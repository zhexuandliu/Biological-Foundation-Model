{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhexuandliu/Biological-LM/blob/main/Protein_Language_Model_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Protein Language Model Demo\n",
        "\n",
        "This notebook accompanies the paper \"Primer on Language Models for Biological Research\". Here, we demonstrate three approaches for applying a pre-trained protein language model to downstream applications:\n",
        "\n",
        "1. Direct Prediction\n",
        "\n",
        "2. Embedding Analysis\n",
        "\n",
        "3. Transfer Learning\n",
        "\n",
        "This illustrates these methods in the context of a protein language model (ESM2), but the general methodologies apply across any natural language or biological language model.\n",
        "\n",
        "The code below was adapted from the HuggingFace tutorial for ESM. See the [original notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/protein_language_modeling.ipynb) for more examples using this model."
      ],
      "metadata": {
        "id": "CV8zJNYLuYzq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup\n",
        "\n",
        "We start with a bit of setup, including installing dependencies, importing packages, loading data, and loading models."
      ],
      "metadata": {
        "id": "k1blrtXgIztX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install dependencies"
      ],
      "metadata": {
        "id": "sehCS5fyvOMw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets evaluate fair-esm pandas torch transformers[torch] umap-learn xgboost"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6oT8CpAoAIpK",
        "outputId": "4cb853c2-5642-42f4-bc25-c7d44098de47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting evaluate\n",
            "  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fair-esm\n",
            "  Downloading fair_esm-2.0.0-py3-none-any.whl (93 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.1/93.1 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
            "Collecting umap-learn\n",
            "  Downloading umap-learn-0.5.5.tar.gz (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.9/90.9 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Collecting responses<0.19 (from evaluate)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.2)\n",
            "Collecting accelerate>=0.21.0 (from transformers[torch])\n",
            "  Downloading accelerate-0.28.0-py3-none-any.whl (290 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.1/290.1 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.2.2)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (0.58.1)\n",
            "Collecting pynndescent>=0.5 (from umap-learn)\n",
            "  Downloading pynndescent-0.5.11-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.2->umap-learn) (0.41.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from pynndescent>=0.5->umap-learn) (1.3.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22->umap-learn) (3.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Building wheels for collected packages: umap-learn\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for umap-learn: filename=umap_learn-0.5.5-py3-none-any.whl size=86832 sha256=6e12fd486a865dde50ead368d5f0843e41c1366fd95aadd30b8c645b8da7ff4b\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/70/07/428d2b58660a1a3b431db59b806a10da736612ebbc66c1bcc5\n",
            "Successfully built umap-learn\n",
            "Installing collected packages: fair-esm, xxhash, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, dill, responses, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, pynndescent, nvidia-cusolver-cu12, umap-learn, datasets, evaluate, accelerate\n",
            "Successfully installed accelerate-0.28.0 datasets-2.18.0 dill-0.3.8 evaluate-0.4.1 fair-esm-2.0.0 multiprocess-0.70.16 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105 pynndescent-0.5.11 responses-0.18.0 umap-learn-0.5.5 xxhash-3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import packages"
      ],
      "metadata": {
        "id": "zJfJun96vYrG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from esm import ESM2, Alphabet, BatchConverter\n",
        "from esm.pretrained import esm2_t6_8M_UR50D\n",
        "from evaluate import load\n",
        "from time import time\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer\n",
        "from tqdm import trange\n",
        "from umap import UMAP\n",
        "from xgboost import XGBClassifier"
      ],
      "metadata": {
        "id": "rnJuwgAtCHVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load data\n",
        "\n",
        "For this demo, we use a dataset of 5,132 human proteins of length 80-500 downloaded from UniProt. Each protein included has a subcellular localization of either the cytosol or cell membrane. We use this data to try to classify whether each sequence is localized within the cytosol (label = 0) or in the membrane (label = 1).\n",
        "\n",
        "In the code below we randomly sample 100 examples from this dataset to reduce the time required to run the notebook. If adapting this for a non-toy example, this should be skipped.\n",
        "\n",
        "See the [HuggingFace tutorial](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/protein_language_modeling.ipynb) for a walk-through of the dataset generation process."
      ],
      "metadata": {
        "id": "CxaQ78zjChJA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download train and test data\n",
        "!wget https://raw.githubusercontent.com/swansonk14/language_models_biology/main/membrane_vs_cytosol_train.csv\n",
        "!wget https://raw.githubusercontent.com/swansonk14/language_models_biology/main/membrane_vs_cytosol_test.csv\n",
        "\n",
        "# Load train and test data\n",
        "train = pd.read_csv(\"membrane_vs_cytosol_train.csv\")\n",
        "test = pd.read_csv(\"membrane_vs_cytosol_test.csv\")\n",
        "\n",
        "# Randomly sample 50 examples from each of train/test to reduce run-time for example\n",
        "n_sample = 50\n",
        "train = train.sample(n=n_sample, random_state=0, replace=False)\n",
        "test = test.sample(n=n_sample, random_state=0, replace=False)\n",
        "\n",
        "# Extract lists of sequences and cytosol/membrane labels\n",
        "train_sequences = train[\"sequences\"].tolist()\n",
        "train_labels = train[\"labels\"].tolist()\n",
        "\n",
        "test_sequences = test[\"sequences\"].tolist()\n",
        "test_labels = test[\"labels\"].tolist()\n",
        "\n",
        "# Combine train and test sequences/labels for some downstream applications\n",
        "sequences = train_sequences + test_sequences\n",
        "labels = np.array(train_labels + test_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3z-kpBM-8NP",
        "outputId": "3c1178ad-8fc2-4237-c49c-29d0dfb12ef8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-18 01:22:50--  https://raw.githubusercontent.com/swansonk14/language_models_biology/main/membrane_vs_cytosol_train.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1140911 (1.1M) [text/plain]\n",
            "Saving to: ‘membrane_vs_cytosol_train.csv.3’\n",
            "\n",
            "membrane_vs_cytosol 100%[===================>]   1.09M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2024-03-18 01:22:50 (22.0 MB/s) - ‘membrane_vs_cytosol_train.csv.3’ saved [1140911/1140911]\n",
            "\n",
            "--2024-03-18 01:22:50--  https://raw.githubusercontent.com/swansonk14/language_models_biology/main/membrane_vs_cytosol_test.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 380710 (372K) [text/plain]\n",
            "Saving to: ‘membrane_vs_cytosol_test.csv.3’\n",
            "\n",
            "membrane_vs_cytosol 100%[===================>] 371.79K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2024-03-18 01:22:51 (10.4 MB/s) - ‘membrane_vs_cytosol_test.csv.3’ saved [380710/380710]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load model\n",
        "\n",
        "Here, we load the smallest ESM2 model, which is a 6-layer Transformer with 8 million parameters. If you want to use a different model, import it from `esm.pretrained` above and call it here instead of `esm2_t6_8M_UR50D`. To clarify the model naming convention:\n",
        "* `t6` The model is a transformer with 6 layers\n",
        "* `8M` There are 8 million parameters in the model\n",
        "* `UR50D` The training data is sequences from UniRef50"
      ],
      "metadata": {
        "id": "1XSdjd1xwsE0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model and associated objects\n",
        "model, alphabet = esm2_t6_8M_UR50D()\n",
        "batch_converter = alphabet.get_batch_converter()\n",
        "model.eval()\n",
        "\n",
        "# Define the final layer of the model\n",
        "# If you select a model with a different number of layers, change this value\n",
        "final_layer = 6"
      ],
      "metadata": {
        "id": "O5aezTRAI7fR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Method #1: Direct Prediction\n",
        "\n",
        "The direct prediction method uses the pre-trained model as is (without any additional training) to make predictions.\n",
        "\n",
        "For a protein language model like ESM2, the model predicts an output for each amino acid in the protein sequence. This output is the predicted probability that each of the 20 possible amino acids would be found at this position. The direct prediction method can either use these probabilities, or it can use the most probable (\"predicted\") amino acid at each position, depending on the application.\n",
        "\n",
        "Below, we are interested in determining whether a single point mutation to a protein sequence is pathogenic or benign. There are multiple ways to use a protein language model for this task. In this tutorial, we approach this by masking$^*$ out the position we are interested in and then querying ESM2 for the probabilities of the wildtype and mutant amino acids at that position. Calculating the log likelihood ratio, LLR = `log(prob(mutant) / prob(wildtype))`, gives the relative likelihood of the mutant form compared to the wildtype form. Prior work has shown that these likelihood ratios are predictive of mutation effect and pathogenicity (https://www.biorxiv.org/content/10.1101/2021.07.09.450648v2, https://www.nature.com/articles/s41588-023-01465-0).\n",
        "\n",
        "$^*$See primer for definition and details on masking in languge models."
      ],
      "metadata": {
        "id": "9zReXLwWINaX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to run ESM2 and compute probabilities\n",
        "def get_esm_probabilities(\n",
        "    model: ESM2,\n",
        "    batch_converter: BatchConverter,\n",
        "    sequences: list[str],\n",
        "    batch_size: int = 2\n",
        ") -> torch.FloatTensor:\n",
        "    \"\"\"Given amino acid sequences, uses an ESM2 model to compute the probability\n",
        "    of each amino acid at each position in each sequence.\n",
        "\n",
        "    :param model: A pre-trained ESM2 model.\n",
        "    :param batch_converter: An ESM2 batch converter.\n",
        "    :param sequences: A list of protein sequences.\n",
        "    :param batch_size: The batch size (# of protein sequences per batch).\n",
        "    :return: A Torch FloatTensor of size (num_sequences, sequence_length, num_amino_acids)\n",
        "      containing the probability of each amino acid at each position in each sequence.\n",
        "    \"\"\"\n",
        "    # Convert protein sequences to (name, sequence) format expected by ESM2\n",
        "    name_sequences = [(f\"protein_{i}\", sequence) for i, sequence in enumerate(sequences)]\n",
        "\n",
        "    # Set up list to store computed probabilities\n",
        "    all_sequence_probabilities = []\n",
        "\n",
        "    # Make probability predictions on all sequences\n",
        "    with torch.no_grad():\n",
        "        # Iterate over batches of sequences\n",
        "        for i in trange(0, len(name_sequences), batch_size):\n",
        "            # Extract batch data\n",
        "            batch_name_sequences = name_sequences[i:i + batch_size]\n",
        "\n",
        "            # Convert batch data to ESM2 format\n",
        "            batch_labels, batch_strs, batch_tokens = batch_converter(batch_name_sequences)\n",
        "\n",
        "            # Compute logits for the sequences\n",
        "            logits = model(batch_tokens, return_contacts=False)[\"logits\"]\n",
        "\n",
        "            # Apply softmax to get amino acid probabilities\n",
        "            probabilities = torch.softmax(logits, dim=-1)\n",
        "\n",
        "            # Save probabilities\n",
        "            all_sequence_probabilities.append(probabilities)\n",
        "\n",
        "    # Concatenate probabilities across batches\n",
        "    all_sequence_probabilities = torch.cat(all_sequence_probabilities)\n",
        "\n",
        "    return all_sequence_probabilities"
      ],
      "metadata": {
        "id": "JtI9H-JSJFSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this example we sample the first protein in our training dataset. This corresponds to Nucleosome assembly protein 1-like 4, a protein that can shuttle between the cytoplasm and nucleus interacting with histones (see [UniProt](https://www.uniprot.org/uniprotkb/Q99733/entry) or [NCBI](https://www.ncbi.nlm.nih.gov/gene/4676) for more details on molecular function or the [UniProt variant viewer](https://www.uniprot.org/uniprotkb/Q99733/variant-viewer) for details on measured and predicted consequences of varitants)."
      ],
      "metadata": {
        "id": "vyf00bVaKBG4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select a protein sequence\n",
        "original_sequence = train_sequences[0]\n",
        "\n",
        "# Choose a position to analyze\n",
        "mask_index = 3\n",
        "\n",
        "# Replace the chosen amino acid with a mask token\n",
        "masked_sequence = original_sequence[:mask_index] + \"<mask>\" + original_sequence[mask_index + 1:]\n",
        "\n",
        "# Compute ESM probabilities for the masked sequence\n",
        "all_probs = get_esm_probabilities(\n",
        "    model=model,\n",
        "    batch_converter=batch_converter,\n",
        "    sequences=[masked_sequence]\n",
        ")\n",
        "\n",
        "# Extract probabilities for the first sequence. Can extract probabilities for other sequences by changing the index.\n",
        "\n",
        "protein_probs = all_probs[0]\n",
        "\n",
        "print()\n",
        "print(f\"The input protein has {len(train_sequences[0])} amino acids and \" +\n",
        "      f\"the shape of the model's output is {protein_probs.numpy().shape}\")"
      ],
      "metadata": {
        "id": "--Rhb874JELB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd85d3cd-4ed0-4c81-c15a-8692466b21f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  2.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "The input protein has 375 amino acids and the shape of the model's output is (377, 33)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the final output is 377 (length of the protein sequence + 2) by 33 (number of standard amino acids + 13). The 13 extra types of tokens are because a protein sequence can includes a few non-standard amino acids (X,B,U,Z,O), some gap/deletions tokens (., -) and other tokens useful for training (e.g. start and end of sequences, etc.). Similarly, the two extra tokens added to our particular sequence correspond to the beginning and end of the sequence. We do not require any 'padding' tokens as we only have 1 sequence, but we ran multiple sequences through the model at once, it would pad them to be a fixed length.\n",
        "\n",
        "For more information on these tokens feel free to explore the attributes in the `alphabet` object. Below see that we enumerate through `alphabet.stadard_toks` which lists all of the amino acid tokens, but you can also explore `alphabet.all_toks`, which lists all extra tokens too."
      ],
      "metadata": {
        "id": "b19bwD7S90ab"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we find the index within the list of 33 possible tokens that corresponds to our wild-type (wt) amino acid, the one that was originally present in our sequence. Then we compare the model's output for all other amino acids against that one. In this case, our wt amino acid is Histidine. If the log-likelihood ratio is < 0, the mutation is predicted to be potentially deleterious."
      ],
      "metadata": {
        "id": "PWk49S2H9_tC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify token index for wt amino acid\n",
        "wildtype_amino_acid = original_sequence[mask_index]\n",
        "wildtype_amino_acid_index = alphabet.tok_to_idx[wildtype_amino_acid]\n",
        "\n",
        "# For each amino acid in the list of 'standard tokens', calculate LLR against wt\n",
        "mutant_llr_ratios = []\n",
        "for character in alphabet.standard_toks:\n",
        "  mutant_amino_acid_index = alphabet.tok_to_idx[character]\n",
        "\n",
        "  mutant_llr_ratios.append(torch.log(\n",
        "    protein_probs[mask_index, mutant_amino_acid_index] /\n",
        "    protein_probs[mask_index, wildtype_amino_acid_index]\n",
        "  ).item())\n",
        "\n",
        "# Plot results\n",
        "f = sns.barplot(mutant_llr_ratios)\n",
        "f.set_xticks(range(len(mutant_llr_ratios)))\n",
        "f.set_xticklabels(alphabet.standard_toks)\n",
        "plt.xlabel(\"Amino Acid\")\n",
        "plt.ylabel(\"Log likelihood ratio (mutant / wildtype)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "80h4USN1_jFh",
        "outputId": "c5983429-a838-4cc3-9c27-416e981648cf"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Method #2: Embedding Analysis\n",
        "\n",
        "The embedding analysis approach, like the direct prediction method, uses the pre-trained model as is. However, rather than obtaining probability or token predictions, the embedding analysis method uses the embeddings (an internal numerical representation) of the protein sequences computed by the model for downstream analyses such as visualization and clustering.\n",
        "\n",
        "Below, we are interested in visualizing the embeddings of protein sequences computed by ESM2 to determine if they can characterize the localization (cytosol or membrane) of proteins. We will first embed all of our protein sequences, by passing the sequences (with no masks) through the model and extracting the internal representation. There are multiple layers at which an embedding _could_ be extracted, here we use the final layer of the model.\n",
        "\n",
        "These embeddings are originally high dimensional (e.g. in the model used here, each sequence is represented by a list of 320 numbers). In order easily visualize them, we use UMAP to reduce the embeddings to 2 dimensions. Finally we will plot the 2-dimensional embeddings and color-code them by protein localization to see whether any patterns emerge."
      ],
      "metadata": {
        "id": "Wr_EWVAkJAaY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_esm_embeddings(\n",
        "        model: ESM2,\n",
        "        batch_converter: BatchConverter,\n",
        "        sequences: list[str],\n",
        "        embedding_layer: int,\n",
        "        average_embeddings: bool = False,\n",
        "        batch_size: int = 2\n",
        ") -> torch.FloatTensor:\n",
        "    \"\"\"Generate embeddings of protein sequences using an ESM2 model.\n",
        "\n",
        "    :param model: A pre-trained ESM2 model.\n",
        "    :param batch_converter: An ESM2 batch converter.\n",
        "    :param sequences: A list of tuples of (name, sequence) for the proteins.\n",
        "    :param embedding_layer: Layer of the ESM2 model from which to will extract embeddings.\n",
        "    :param average_embeddings: Whether to average the residue embeddings for each protein.\n",
        "    :param batch_size: The batch size (# of protein sequences per batch).\n",
        "    :return: A Torch FloatTensor of sequence embeddings.\n",
        "      If average_embeddings=True, size is (num_sequences, embedding_size).\n",
        "      If average_embeddings=False, size is (num_sequences, sequence_length, embedding_size).\n",
        "    \"\"\"\n",
        "    # Convert protein sequences to (name, sequence) format expected by ESM2\n",
        "    name_sequences = [(f\"protein_{i}\", sequence) for i, sequence in enumerate(sequences)]\n",
        "\n",
        "    # Set up list to store computed embeddings\n",
        "    embeddings = []\n",
        "\n",
        "    # Make probability predictions on all sequences\n",
        "    with torch.no_grad():\n",
        "        # Iterate over batches of sequences\n",
        "        for i in trange(0, len(sequences), batch_size):\n",
        "            # Extract batch data\n",
        "            batch_name_sequences = name_sequences[i:i + batch_size]\n",
        "\n",
        "            # Convert batch data to ESM2 format\n",
        "            batch_labels, batch_strs, batch_tokens = batch_converter(batch_name_sequences)\n",
        "\n",
        "            # Calculate the length of each sequence in a batch\n",
        "            batch_lengths = (batch_tokens != alphabet.padding_idx).sum(1)\n",
        "\n",
        "            # Subtract 2 from each to remove start and end tokens\n",
        "            batch_lengths -= 2\n",
        "\n",
        "            # Compute embeddings for the sequences\n",
        "            results = model(batch_tokens, repr_layers=[embedding_layer], return_contacts=False)\n",
        "\n",
        "            # Get per-amino acid embeddings for the selected layer\n",
        "            batch_embeddings = results['representations'][embedding_layer]\n",
        "\n",
        "            # Extract embeddings and optionally average across the sequence\n",
        "            for (name, _,), embedding, sequence_len in zip(batch_name_sequences, batch_embeddings, batch_lengths):\n",
        "                # Extract embeddings for amino acids without start, end, or pad tokens\n",
        "                embedding = embedding[1 : sequence_len + 1]\n",
        "\n",
        "                # Optionally, average embeddings across amino acids in the sequence\n",
        "                if average_embeddings:\n",
        "                    embedding = embedding.mean(dim=0)\n",
        "\n",
        "                # Save embeddings\n",
        "                embeddings.append(embedding)\n",
        "\n",
        "    # Stack embeddings across sequences\n",
        "    embeddings = torch.stack(embeddings)\n",
        "\n",
        "    return embeddings"
      ],
      "metadata": {
        "id": "6uzy2cs7B2-F"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute embeddings for all sequences (with averaging to give a single embedding per sequence)\n",
        "# This should take about 40 seconds for 50 sequences on the default Colab CPU\n",
        "embeddings = generate_esm_embeddings(\n",
        "    model=model,\n",
        "    batch_converter=batch_converter,\n",
        "    sequences=sequences,\n",
        "    embedding_layer=final_layer,\n",
        "    average_embeddings=True\n",
        ")\n",
        "\n",
        "print(embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3yFVYZXU_hhq",
        "outputId": "48fdf5da-8efb-45e9-d2a3-3d81cf4aea1a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [00:53<00:00,  1.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([100, 320])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see the embeddings have a shape of 200 (number of sequences) x 320 (embedding dimension). This embedding dimension depends on your model of choice so if you use a model other than `esm_t6_8M_UR50D`, the embedding dimension might differ."
      ],
      "metadata": {
        "id": "MPjeASQwA5Rh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply UMAP to reduce the embeddings to two dimensions\n",
        "embeddings_2d = UMAP(random_state=0).fit_transform(embeddings)\n",
        "\n",
        "# Plot the UMAP of embeddings, colored by cytosol/membrane label\n",
        "sns.scatterplot(\n",
        "    x=embeddings_2d[labels == 0, 0],\n",
        "    y=embeddings_2d[labels == 0, 1],\n",
        "    label=\"Cytosol\"\n",
        ")\n",
        "sns.scatterplot(\n",
        "    x=embeddings_2d[labels == 1, 0],\n",
        "    y=embeddings_2d[labels == 1, 1],\n",
        "    label=\"Membrane\"\n",
        ")\n",
        "\n",
        "plt.xlabel(\"UMAP 0\")\n",
        "plt.ylabel(\"UMAP 1\")\n",
        "\n",
        "plt.xticks([])\n",
        "plt.yticks([])\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "u6FFeDmm5QBF",
        "outputId": "a156dc8a-b13e-47bc-a2ba-5725a829e46d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Method #3: Transfer Learning\n",
        "\n",
        "The transfer learning method starts with the pre-trained language model but applies additional training to adapt the model to a particular downstream application.\n",
        "\n",
        "There are two general techniques for applying transfer learning.\n",
        "\n",
        "1. **Train on Embeddings:** Given limited data and compute, one transfer learning approach is to use the pre-trained model to compute embeddings for each sequence and then train a separate, typically smaller, model to predict the downstream task from those embeddings.\n",
        "\n",
        "2. **Fine-Tune:** Given sufficient data and compute, another transfer learning method is to fine-tune (further train) the language model itself. This could involve fine-tuning some or all of the layers of the model.\n",
        "\n",
        "Below, we demonstrate both transfer learning techniques for predicting protein localization"
      ],
      "metadata": {
        "id": "mbxg0wpY6tFd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Method 3-1: Train on Embeddings\n",
        "\n",
        "For this transfer learning method, we will use the protein sequence embeddings we previously computed in Method 2 and train a small XGBoost model with those embeddings as input to predict protein localization."
      ],
      "metadata": {
        "id": "b8iCkf1wIq9M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a function to compute accuracy\n",
        "accuracy = load(\"accuracy\")"
      ],
      "metadata": {
        "id": "ISMFDW3pJ8fa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate the embeddings into train and test\n",
        "train_embeddings = embeddings[:len(train_sequences)]\n",
        "test_embeddings = embeddings[len(train_sequences):]"
      ],
      "metadata": {
        "id": "S2nLMh-vJPbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train an XGB classifier on the training data (embeddings and labels)\n",
        "xgb_model = XGBClassifier(random_state=0).fit(train_embeddings, train_labels)"
      ],
      "metadata": {
        "id": "vEgdbE8fIvex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the trained XGB classifier to make predictions on the test data\n",
        "test_preds = xgb_model.predict(test_embeddings)"
      ],
      "metadata": {
        "id": "TX6RA_Sc8_pQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the XGB test predictions\n",
        "print(f\"XGB test accuracy = {accuracy.compute(predictions=test_preds, references=test_labels)['accuracy']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMRZCVbgJepE",
        "outputId": "0a5cad1e-6d2c-4900-ae47-e9bbcad04e62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XGB test accuracy = 0.84\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Method 3-2: Fine-Tune\n",
        "\n",
        "For this transfer learning method, we will fine-tune the entire ESM2 model to predict protein localization.\n",
        "\n",
        "Since our protein localization model requires a classification layer at the end of the model, we will load the ESM2 model using an alternate mechanism that replaces the final layer of the model (which typically predicts an amino acid _per position_ of the sequence) with a classification layer that predicts 1 value _per sequence_.\n",
        "\n",
        "This should take about 4.5 minutes to train in the Colab.\n",
        "\n",
        "\n",
        "**Technical Notes**\n",
        "\n",
        "When fine-tuning we need to explicitely define our _tokenizer_. A tokenizer turns a protein representation from a sequence of strings into a sequence of \"tokens\" (integer numbers) that can be fed into the network. In the earlier example, the tokenization process is abstracted away but here we must run it manually."
      ],
      "metadata": {
        "id": "pUezFIV_IuFc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the ESM2 model\n",
        "model_name = \"esm2_t6_8M_UR50D\"\n",
        "model_checkpoint = f\"facebook/{model_name}\"\n",
        "\n",
        "# Load a tokenizer that converts amino acids into the format expected by ESM2\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "# Load the ESM2 model\n",
        "# Note: Ignore warnings some weights not being initialized since we need this for fine-tuning\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=2)\n",
        "\n",
        "# Tokenize the train and test sequences\n",
        "train_tokenized = tokenizer(train[\"sequences\"].tolist())\n",
        "test_tokenized = tokenizer(test[\"sequences\"].tolist())\n",
        "\n",
        "# Convert the train and test sequences into a Dataset object\n",
        "train_dataset = Dataset.from_dict(train_tokenized)\n",
        "test_dataset = Dataset.from_dict(test_tokenized)\n",
        "\n",
        "# Add labels to the train and test datasets\n",
        "train_dataset = train_dataset.add_column(\"labels\", train[\"labels\"].tolist())\n",
        "test_dataset = test_dataset.add_column(\"labels\", test[\"labels\"].tolist())\n",
        "\n",
        "# Define function to compute accuracy for model evaluation\n",
        "def compute_accuracy(eval_pred: tuple[np.ndarray, np.ndarray]) -> float:\n",
        "    \"\"\"Computes the accuracy of predictions.\n",
        "\n",
        "    :param eval_pred: A tuple of NumPy arrays containing an array of predictions\n",
        "      and an array of labels.\n",
        "    :return: The accuracy of the predictions.\n",
        "    \"\"\"\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return accuracy.compute(predictions=predictions, references=labels)\n",
        "\n",
        "# Define the ESM2 training arguments\n",
        "args = TrainingArguments(\n",
        "    f\"{model_name}-finetuned-localization\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\"\n",
        ")\n",
        "\n",
        "# Set up the Trainer for training the ESM2 model\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_accuracy,\n",
        ")\n",
        "\n",
        "# Fine-tune the ESM2 model and evaluate prediction accuracy\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "D34Ug78H95hR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "outputId": "d54b0ca3-ead9-44b1-9254-b7ccfc0d36f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of EsmForSequenceClassification were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='39' max='39' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [39/39 04:28, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.663123</td>\n",
              "      <td>0.800000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.638601</td>\n",
              "      <td>0.820000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.629226</td>\n",
              "      <td>0.840000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=39, training_loss=0.6543462704389523, metrics={'train_runtime': 272.8323, 'train_samples_per_second': 0.55, 'train_steps_per_second': 0.143, 'total_flos': 2690325281460.0, 'train_loss': 0.6543462704389523, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "In this toy example, we see that both embedding the sequences training on embeddings and fine-tuning the full model are both effective at predicting subcellular localization of these cytosolic and membrane proteins. As we saw in the embedding visualization, the internal embedding of the model already separates many of these proteins, so the extra learning task is not the most challenging. We also see that the fine-tuning method is both more complicated fro the user and takes quite a bit longer than training on the embedding. For this example task, the extra effort and time do improve the performance. However, this can be quite task-dependent, and for more complicated tasks, the extra computational power can be critical."
      ],
      "metadata": {
        "id": "b81DK3tmNnyO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-RX4F-74RHID"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}