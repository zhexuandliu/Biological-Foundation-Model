{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Distribution Analysis for scGPT Fine-tuning: Training vs Testing Data\n",
        "\n",
        "This notebook analyzes the distribution differences between training and testing data in the scGPT fine-tuning pipeline to investigate potential out-of-distribution (OOD) issues that might affect model generalization.\n",
        "\n",
        "## Overview\n",
        "- **Goal**: Verify if fine-tuning performance issues are due to distribution shifts between train/test data\n",
        "- **Dataset**: Adamson perturbation data with simulation split\n",
        "- **Analysis**: Statistical and visual comparison of gene expression patterns, perturbation effects, and cell characteristics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup and Installation for Google Colab\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "os.chdir('/content/drive/MyDrive/GitHub/Biological-Foundation-Model/notebooks/scGPT_finetune')\n",
        "\n",
        "# Install required packages\n",
        "%pip install -r ./requirements.txt\n",
        "%pip install scgpt \"flash-attn<1.0.5\"\n",
        "%pip install seaborn plotly scipy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import json\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import copy\n",
        "from pathlib import Path\n",
        "from typing import Iterable, List, Tuple, Dict, Union, Optional\n",
        "import warnings\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "from scipy import stats\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.metrics import pairwise_distances\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torchtext.vocab import Vocab\n",
        "from torchtext._torchtext import (\n",
        "    Vocab as VocabPybind,\n",
        ")\n",
        "from torch_geometric.loader import DataLoader\n",
        "from gears import PertData, GEARS\n",
        "from gears.inference import compute_metrics, deeper_analysis, non_dropout_analysis\n",
        "from gears.utils import create_cell_graph_dataset_for_prediction\n",
        "\n",
        "sys.path.insert(0, \"../\")\n",
        "\n",
        "import scgpt as scg\n",
        "from scgpt.model import TransformerGenerator\n",
        "from scgpt.loss import (\n",
        "    masked_mse_loss,\n",
        "    criterion_neg_log_bernoulli,\n",
        "    masked_relative_error,\n",
        ")\n",
        "from scgpt.tokenizer import tokenize_batch, pad_batch, tokenize_and_pad_batch\n",
        "from scgpt.tokenizer.gene_tokenizer import GeneVocab\n",
        "from scgpt.utils import set_seed, map_raw_id_to_vocab_id, compute_perturbation_metrics\n",
        "\n",
        "# Set up plotting\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "matplotlib.rcParams[\"savefig.transparent\"] = False\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "set_seed(42)\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and prepare data\n",
        "print(\"Loading perturbation data...\")\n",
        "\n",
        "# Settings for data processing\n",
        "pad_token = \"<pad>\"\n",
        "special_tokens = [pad_token, \"<cls>\", \"<eoc>\"]\n",
        "pad_value = 0\n",
        "pert_pad_id = 0\n",
        "include_zero_gene = \"all\"\n",
        "max_seq_len = 1536\n",
        "\n",
        "# Dataset settings\n",
        "data_name = \"adamson\"\n",
        "split = \"simulation\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load perturbation data\n",
        "pert_data = PertData(\"./data\")\n",
        "pert_data.load(data_name=data_name)\n",
        "pert_data.prepare_split(split=split, seed=1)\n",
        "pert_data.get_dataloader(batch_size=64, test_batch_size=64)\n",
        "\n",
        "print(f\"Data loaded successfully!\")\n",
        "print(f\"Dataset: {data_name}\")\n",
        "print(f\"Split: {split}\")\n",
        "print(f\"Device: {device}\")\n",
        "\n",
        "# Get basic info about the dataset\n",
        "adata = pert_data.adata\n",
        "print(f\"\\nDataset info:\")\n",
        "print(f\"Total cells: {adata.n_obs}\")\n",
        "print(f\"Total genes: {adata.n_vars}\")\n",
        "print(f\"Conditions: {adata.obs['condition'].unique()}\")\n",
        "print(f\"Split composition: {pert_data.subgroup}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract train/test splits for analysis\n",
        "def extract_split_data(adata, split_info, split_name):\n",
        "    \"\"\"Extract data for a specific split\"\"\"\n",
        "    if split_name == \"train\":\n",
        "        split_cells = split_info[\"train_idx\"]\n",
        "    elif split_name == \"test\":\n",
        "        split_cells = split_info[\"test_idx\"]\n",
        "    elif split_name == \"val\":\n",
        "        split_cells = split_info[\"val_idx\"]\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown split: {split_name}\")\n",
        "    \n",
        "    return adata[split_cells].copy()\n",
        "\n",
        "# Get split information\n",
        "train_adata = extract_split_data(adata, pert_data.split, \"train\")\n",
        "test_adata = extract_split_data(adata, pert_data.split, \"test\")\n",
        "val_adata = extract_split_data(adata, pert_data.split, \"val\")\n",
        "\n",
        "print(f\"Split sizes:\")\n",
        "print(f\"Train: {train_adata.n_obs} cells\")\n",
        "print(f\"Test: {test_adata.n_obs} cells\") \n",
        "print(f\"Val: {val_adata.n_obs} cells\")\n",
        "\n",
        "# Get condition distributions\n",
        "print(f\"\\nCondition distributions:\")\n",
        "print(f\"Train conditions: {train_adata.obs['condition'].value_counts()}\")\n",
        "print(f\"Test conditions: {test_adata.obs['condition'].value_counts()}\")\n",
        "print(f\"Val conditions: {val_adata.obs['condition'].value_counts()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Gene Expression Distribution Analysis\n",
        "print(\"=== GENE EXPRESSION DISTRIBUTION ANALYSIS ===\")\n",
        "\n",
        "def analyze_gene_expression_distribution(adata, split_name):\n",
        "    \"\"\"Analyze gene expression distribution for a split\"\"\"\n",
        "    # Convert to dense if sparse\n",
        "    if hasattr(adata.X, 'toarray'):\n",
        "        X = adata.X.toarray()\n",
        "    else:\n",
        "        X = adata.X\n",
        "    \n",
        "    # Calculate statistics\n",
        "    mean_expr = np.mean(X, axis=0)\n",
        "    std_expr = np.std(X, axis=0)\n",
        "    var_expr = np.var(X, axis=0)\n",
        "    \n",
        "    # Calculate per-cell statistics\n",
        "    cell_means = np.mean(X, axis=1)\n",
        "    cell_stds = np.std(X, axis=1)\n",
        "    cell_zeros = np.sum(X == 0, axis=1) / X.shape[1]  # fraction of zeros per cell\n",
        "    \n",
        "    return {\n",
        "        'gene_means': mean_expr,\n",
        "        'gene_stds': std_expr,\n",
        "        'gene_vars': var_expr,\n",
        "        'cell_means': cell_means,\n",
        "        'cell_stds': cell_stds,\n",
        "        'cell_zeros': cell_zeros,\n",
        "        'total_expression': np.sum(X, axis=1),\n",
        "        'n_genes_expressed': np.sum(X > 0, axis=1)\n",
        "    }\n",
        "\n",
        "# Analyze each split\n",
        "train_stats = analyze_gene_expression_distribution(train_adata, \"train\")\n",
        "test_stats = analyze_gene_expression_distribution(test_adata, \"test\")\n",
        "val_stats = analyze_gene_expression_distribution(val_adata, \"val\")\n",
        "\n",
        "print(f\"Gene expression statistics calculated for all splits\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize gene expression distributions\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "fig.suptitle('Gene Expression Distribution Comparison: Train vs Test', fontsize=16)\n",
        "\n",
        "# 1. Gene mean expression\n",
        "axes[0, 0].hist(train_stats['gene_means'], bins=50, alpha=0.7, label='Train', density=True)\n",
        "axes[0, 0].hist(test_stats['gene_means'], bins=50, alpha=0.7, label='Test', density=True)\n",
        "axes[0, 0].set_xlabel('Mean Gene Expression')\n",
        "axes[0, 0].set_ylabel('Density')\n",
        "axes[0, 0].set_title('Gene Mean Expression Distribution')\n",
        "axes[0, 0].legend()\n",
        "\n",
        "# 2. Gene expression variance\n",
        "axes[0, 1].hist(train_stats['gene_vars'], bins=50, alpha=0.7, label='Train', density=True)\n",
        "axes[0, 1].hist(test_stats['gene_vars'], bins=50, alpha=0.7, label='Test', density=True)\n",
        "axes[0, 1].set_xlabel('Gene Expression Variance')\n",
        "axes[0, 1].set_ylabel('Density')\n",
        "axes[0, 1].set_title('Gene Expression Variance Distribution')\n",
        "axes[0, 1].legend()\n",
        "\n",
        "# 3. Cell mean expression\n",
        "axes[0, 2].hist(train_stats['cell_means'], bins=50, alpha=0.7, label='Train', density=True)\n",
        "axes[0, 2].hist(test_stats['cell_means'], bins=50, alpha=0.7, label='Test', density=True)\n",
        "axes[0, 2].set_xlabel('Mean Expression per Cell')\n",
        "axes[0, 2].set_ylabel('Density')\n",
        "axes[0, 2].set_title('Cell Mean Expression Distribution')\n",
        "axes[0, 2].legend()\n",
        "\n",
        "# 4. Fraction of zeros per cell\n",
        "axes[1, 0].hist(train_stats['cell_zeros'], bins=50, alpha=0.7, label='Train', density=True)\n",
        "axes[1, 0].hist(test_stats['cell_zeros'], bins=50, alpha=0.7, label='Test', density=True)\n",
        "axes[1, 0].set_xlabel('Fraction of Zeros per Cell')\n",
        "axes[1, 0].set_ylabel('Density')\n",
        "axes[1, 0].set_title('Sparsity Distribution')\n",
        "axes[1, 0].legend()\n",
        "\n",
        "# 5. Number of genes expressed per cell\n",
        "axes[1, 1].hist(train_stats['n_genes_expressed'], bins=50, alpha=0.7, label='Train', density=True)\n",
        "axes[1, 1].hist(test_stats['n_genes_expressed'], bins=50, alpha=0.7, label='Test', density=True)\n",
        "axes[1, 1].set_xlabel('Number of Genes Expressed')\n",
        "axes[1, 1].set_ylabel('Density')\n",
        "axes[1, 1].set_title('Genes Expressed per Cell')\n",
        "axes[1, 1].legend()\n",
        "\n",
        "# 6. Total expression per cell\n",
        "axes[1, 2].hist(train_stats['total_expression'], bins=50, alpha=0.7, label='Train', density=True)\n",
        "axes[1, 2].hist(test_stats['total_expression'], bins=50, alpha=0.7, label='Test', density=True)\n",
        "axes[1, 2].set_xlabel('Total Expression per Cell')\n",
        "axes[1, 2].set_ylabel('Density')\n",
        "axes[1, 2].set_title('Total Expression Distribution')\n",
        "axes[1, 2].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Statistical tests for distribution differences\n",
        "print(\"=== STATISTICAL TESTS FOR DISTRIBUTION DIFFERENCES ===\")\n",
        "\n",
        "def perform_statistical_tests(train_stats, test_stats, metric_name):\n",
        "    \"\"\"Perform statistical tests comparing train and test distributions\"\"\"\n",
        "    train_data = train_stats[metric_name]\n",
        "    test_data = test_stats[metric_name]\n",
        "    \n",
        "    # Kolmogorov-Smirnov test\n",
        "    ks_stat, ks_pvalue = stats.ks_2samp(train_data, test_data)\n",
        "    \n",
        "    # Mann-Whitney U test (non-parametric)\n",
        "    mw_stat, mw_pvalue = stats.mannwhitneyu(train_data, test_data, alternative='two-sided')\n",
        "    \n",
        "    # Effect size (Cohen's d)\n",
        "    pooled_std = np.sqrt(((len(train_data) - 1) * np.var(train_data) + \n",
        "                         (len(test_data) - 1) * np.var(test_data)) / \n",
        "                        (len(train_data) + len(test_data) - 2))\n",
        "    cohens_d = (np.mean(train_data) - np.mean(test_data)) / pooled_std\n",
        "    \n",
        "    return {\n",
        "        'ks_stat': ks_stat,\n",
        "        'ks_pvalue': ks_pvalue,\n",
        "        'mw_stat': mw_stat,\n",
        "        'mw_pvalue': mw_pvalue,\n",
        "        'cohens_d': cohens_d,\n",
        "        'train_mean': np.mean(train_data),\n",
        "        'test_mean': np.mean(test_data),\n",
        "        'train_std': np.std(train_data),\n",
        "        'test_std': np.std(test_data)\n",
        "    }\n",
        "\n",
        "# Test different metrics\n",
        "metrics_to_test = ['gene_means', 'gene_vars', 'cell_means', 'cell_zeros', \n",
        "                   'n_genes_expressed', 'total_expression']\n",
        "\n",
        "statistical_results = {}\n",
        "for metric in metrics_to_test:\n",
        "    print(f\"\\nTesting {metric}:\")\n",
        "    results = perform_statistical_tests(train_stats, test_stats, metric)\n",
        "    statistical_results[metric] = results\n",
        "    \n",
        "    print(f\"  Train mean: {results['train_mean']:.4f} ± {results['train_std']:.4f}\")\n",
        "    print(f\"  Test mean:  {results['test_mean']:.4f} ± {results['test_std']:.4f}\")\n",
        "    print(f\"  Cohen's d:  {results['cohens_d']:.4f}\")\n",
        "    print(f\"  KS test:    stat={results['ks_stat']:.4f}, p={results['ks_pvalue']:.4f}\")\n",
        "    print(f\"  MW test:    stat={results['mw_stat']:.4f}, p={results['mw_pvalue']:.4f}\")\n",
        "    \n",
        "    # Interpretation\n",
        "    if results['ks_pvalue'] < 0.05:\n",
        "        print(f\"  *** SIGNIFICANT DIFFERENCE (KS test, p < 0.05) ***\")\n",
        "    if abs(results['cohens_d']) > 0.2:\n",
        "        effect_size = \"small\" if abs(results['cohens_d']) < 0.5 else \"medium\" if abs(results['cohens_d']) < 0.8 else \"large\"\n",
        "        print(f\"  *** {effect_size.upper()} EFFECT SIZE (Cohen's d = {results['cohens_d']:.3f}) ***\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. Perturbation Pattern Analysis\n",
        "print(\"\\n=== PERTURBATION PATTERN ANALYSIS ===\")\n",
        "\n",
        "def analyze_perturbation_patterns(train_adata, test_adata):\n",
        "    \"\"\"Analyze perturbation patterns between train and test sets\"\"\"\n",
        "    \n",
        "    # Get perturbation conditions (exclude control)\n",
        "    train_conditions = train_adata.obs['condition'].value_counts()\n",
        "    test_conditions = test_adata.obs['condition'].value_counts()\n",
        "    \n",
        "    # Get unique perturbations\n",
        "    train_perts = set(train_conditions.index) - {'ctrl'}\n",
        "    test_perts = set(test_conditions.index) - {'ctrl'}\n",
        "    \n",
        "    # Analyze perturbation overlap\n",
        "    common_perts = train_perts.intersection(test_perts)\n",
        "    train_only = train_perts - test_perts\n",
        "    test_only = test_perts - train_perts\n",
        "    \n",
        "    # Analyze perturbation types\n",
        "    def categorize_perturbation(pert_name):\n",
        "        if '+' in pert_name:\n",
        "            return 'combinatorial'\n",
        "        else:\n",
        "            return 'single'\n",
        "    \n",
        "    train_pert_types = [categorize_perturbation(p) for p in train_perts]\n",
        "    test_pert_types = [categorize_perturbation(p) for p in test_perts]\n",
        "    \n",
        "    return {\n",
        "        'train_conditions': train_conditions,\n",
        "        'test_conditions': test_conditions,\n",
        "        'common_perts': common_perts,\n",
        "        'train_only': train_only,\n",
        "        'test_only': test_only,\n",
        "        'train_pert_types': train_pert_types,\n",
        "        'test_pert_types': test_pert_types,\n",
        "        'train_single_count': train_pert_types.count('single'),\n",
        "        'train_combo_count': train_pert_types.count('combinatorial'),\n",
        "        'test_single_count': test_pert_types.count('single'),\n",
        "        'test_combo_count': test_pert_types.count('combinatorial')\n",
        "    }\n",
        "\n",
        "pert_analysis = analyze_perturbation_patterns(train_adata, test_adata)\n",
        "\n",
        "print(f\"Perturbation overlap analysis:\")\n",
        "print(f\"  Common perturbations: {len(pert_analysis['common_perts'])}\")\n",
        "print(f\"  Train-only perturbations: {len(pert_analysis['train_only'])}\")\n",
        "print(f\"  Test-only perturbations: {len(pert_analysis['test_only'])}\")\n",
        "\n",
        "print(f\"\\nPerturbation type distribution:\")\n",
        "print(f\"  Train - Single: {pert_analysis['train_single_count']}, Combinatorial: {pert_analysis['train_combo_count']}\")\n",
        "print(f\"  Test - Single: {pert_analysis['test_single_count']}, Combinatorial: {pert_analysis['test_combo_count']}\")\n",
        "\n",
        "print(f\"\\nTest-only perturbations: {list(pert_analysis['test_only'])}\")\n",
        "print(f\"Train-only perturbations: {list(pert_analysis['train_only'])}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. Dimensionality Reduction Analysis\n",
        "print(\"\\n=== DIMENSIONALITY REDUCTION ANALYSIS ===\")\n",
        "\n",
        "def perform_dimensionality_reduction(adata, method='PCA', n_components=50):\n",
        "    \"\"\"Perform dimensionality reduction on gene expression data\"\"\"\n",
        "    # Convert to dense if sparse\n",
        "    if hasattr(adata.X, 'toarray'):\n",
        "        X = adata.X.toarray()\n",
        "    else:\n",
        "        X = adata.X\n",
        "    \n",
        "    if method == 'PCA':\n",
        "        reducer = PCA(n_components=n_components, random_state=42)\n",
        "    elif method == 'tSNE':\n",
        "        reducer = TSNE(n_components=2, random_state=42, perplexity=30)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown method: {method}\")\n",
        "    \n",
        "    embedding = reducer.fit_transform(X)\n",
        "    return embedding, reducer\n",
        "\n",
        "# Perform PCA on each split\n",
        "print(\"Computing PCA embeddings...\")\n",
        "train_pca, train_pca_model = perform_dimensionality_reduction(train_adata, 'PCA', 50)\n",
        "test_pca, test_pca_model = perform_dimensionality_reduction(test_adata, 'PCA', 50)\n",
        "val_pca, val_pca_model = perform_dimensionality_reduction(val_adata, 'PCA', 50)\n",
        "\n",
        "# Perform t-SNE on combined data for visualization\n",
        "print(\"Computing t-SNE embedding...\")\n",
        "combined_adata = train_adata.concatenate([test_adata, val_adata], \n",
        "                                        batch_key='split', \n",
        "                                        batch_categories=['train', 'test', 'val'])\n",
        "combined_tsne, _ = perform_dimensionality_reduction(combined_adata, 'tSNE', 2)\n",
        "\n",
        "print(f\"PCA explained variance ratios (first 10 components):\")\n",
        "print(f\"  Train: {train_pca_model.explained_variance_ratio_[:10]}\")\n",
        "print(f\"  Test:  {test_pca_model.explained_variance_ratio_[:10]}\")\n",
        "print(f\"  Val:   {val_pca_model.explained_variance_ratio_[:10]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize dimensionality reduction results\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "# 1. PCA comparison (first 2 components)\n",
        "axes[0].scatter(train_pca[:, 0], train_pca[:, 1], alpha=0.6, label='Train', s=20)\n",
        "axes[0].scatter(test_pca[:, 0], test_pca[:, 1], alpha=0.6, label='Test', s=20)\n",
        "axes[0].set_xlabel(f'PC1 ({train_pca_model.explained_variance_ratio_[0]:.2%} variance)')\n",
        "axes[0].set_ylabel(f'PC2 ({train_pca_model.explained_variance_ratio_[1]:.2%} variance)')\n",
        "axes[0].set_title('PCA: Train vs Test')\n",
        "axes[0].legend()\n",
        "\n",
        "# 2. t-SNE visualization\n",
        "split_colors = ['red' if s == 'train' else 'blue' if s == 'test' else 'green' for s in combined_adata.obs['split']]\n",
        "axes[1].scatter(combined_tsne[:, 0], combined_tsne[:, 1], c=split_colors, alpha=0.6, s=20)\n",
        "axes[1].set_xlabel('t-SNE 1')\n",
        "axes[1].set_ylabel('t-SNE 2')\n",
        "axes[1].set_title('t-SNE: Train (red) vs Test (blue) vs Val (green)')\n",
        "\n",
        "# 3. PCA variance explained\n",
        "axes[2].plot(range(1, 11), train_pca_model.explained_variance_ratio_[:10], 'o-', label='Train')\n",
        "axes[2].plot(range(1, 11), test_pca_model.explained_variance_ratio_[:10], 's-', label='Test')\n",
        "axes[2].set_xlabel('Principal Component')\n",
        "axes[2].set_ylabel('Explained Variance Ratio')\n",
        "axes[2].set_title('PCA Explained Variance')\n",
        "axes[2].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. Distance-based Analysis\n",
        "print(\"\\n=== DISTANCE-BASED ANALYSIS ===\")\n",
        "\n",
        "def compute_distances_between_splits(train_adata, test_adata, n_samples=1000):\n",
        "    \"\"\"Compute distances between train and test splits\"\"\"\n",
        "    \n",
        "    # Sample cells for computational efficiency\n",
        "    if train_adata.n_obs > n_samples:\n",
        "        train_indices = np.random.choice(train_adata.n_obs, n_samples, replace=False)\n",
        "        train_sample = train_adata[train_indices]\n",
        "    else:\n",
        "        train_sample = train_adata\n",
        "        \n",
        "    if test_adata.n_obs > n_samples:\n",
        "        test_indices = np.random.choice(test_adata.n_obs, n_samples, replace=False)\n",
        "        test_sample = test_adata[test_indices]\n",
        "    else:\n",
        "        test_sample = test_adata\n",
        "    \n",
        "    # Convert to dense matrices\n",
        "    if hasattr(train_sample.X, 'toarray'):\n",
        "        train_X = train_sample.X.toarray()\n",
        "        test_X = test_sample.X.toarray()\n",
        "    else:\n",
        "        train_X = train_sample.X\n",
        "        test_X = test_sample.X\n",
        "    \n",
        "    # Compute pairwise distances within each split\n",
        "    train_distances = pairwise_distances(train_X, metric='cosine')\n",
        "    test_distances = pairwise_distances(test_X, metric='cosine')\n",
        "    \n",
        "    # Compute distances between splits\n",
        "    between_distances = pairwise_distances(train_X, test_X, metric='cosine')\n",
        "    \n",
        "    return {\n",
        "        'train_within': train_distances[np.triu_indices_from(train_distances, k=1)],\n",
        "        'test_within': test_distances[np.triu_indices_from(test_distances, k=1)],\n",
        "        'between': between_distances.flatten()\n",
        "    }\n",
        "\n",
        "print(\"Computing distance matrices...\")\n",
        "distances = compute_distances_between_splits(train_adata, test_adata, n_samples=500)\n",
        "\n",
        "# Visualize distance distributions\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "# Distance distributions\n",
        "axes[0].hist(distances['train_within'], bins=50, alpha=0.7, label='Train within', density=True)\n",
        "axes[0].hist(distances['test_within'], bins=50, alpha=0.7, label='Test within', density=True)\n",
        "axes[0].hist(distances['between'], bins=50, alpha=0.7, label='Train-Test between', density=True)\n",
        "axes[0].set_xlabel('Cosine Distance')\n",
        "axes[0].set_ylabel('Density')\n",
        "axes[0].set_title('Distance Distributions')\n",
        "axes[0].legend()\n",
        "\n",
        "# Box plot comparison\n",
        "data_to_plot = [distances['train_within'], distances['test_within'], distances['between']]\n",
        "labels = ['Train within', 'Test within', 'Train-Test between']\n",
        "axes[1].boxplot(data_to_plot, labels=labels)\n",
        "axes[1].set_ylabel('Cosine Distance')\n",
        "axes[1].set_title('Distance Comparison')\n",
        "axes[1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Statistical comparison\n",
        "print(f\"Distance statistics:\")\n",
        "print(f\"  Train within: {np.mean(distances['train_within']):.4f} ± {np.std(distances['train_within']):.4f}\")\n",
        "print(f\"  Test within:  {np.mean(distances['test_within']):.4f} ± {np.std(distances['test_within']):.4f}\")\n",
        "print(f\"  Between:      {np.mean(distances['between']):.4f} ± {np.std(distances['between']):.4f}\")\n",
        "\n",
        "# Test if between-split distances are significantly different\n",
        "ks_stat, ks_pvalue = stats.ks_2samp(distances['train_within'], distances['between'])\n",
        "print(f\"\\nKS test (train within vs between): stat={ks_stat:.4f}, p={ks_pvalue:.4f}\")\n",
        "if ks_pvalue < 0.05:\n",
        "    print(\"*** SIGNIFICANT DIFFERENCE in distance distributions ***\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5. Summary and Conclusions\n",
        "print(\"\\n=== SUMMARY AND CONCLUSIONS ===\")\n",
        "\n",
        "def generate_summary_report(statistical_results, pert_analysis, distances):\n",
        "    \"\"\"Generate a comprehensive summary report\"\"\"\n",
        "    \n",
        "    print(\"DISTRIBUTION ANALYSIS SUMMARY\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Gene expression differences\n",
        "    print(\"\\n1. GENE EXPRESSION DISTRIBUTION DIFFERENCES:\")\n",
        "    significant_diffs = []\n",
        "    for metric, results in statistical_results.items():\n",
        "        if results['ks_pvalue'] < 0.05:\n",
        "            significant_diffs.append(metric)\n",
        "            effect_size = \"small\" if abs(results['cohens_d']) < 0.5 else \"medium\" if abs(results['cohens_d']) < 0.8 else \"large\"\n",
        "            print(f\"   - {metric}: SIGNIFICANT (p={results['ks_pvalue']:.4f}, {effect_size} effect, d={results['cohens_d']:.3f})\")\n",
        "        else:\n",
        "            print(f\"   - {metric}: Not significant (p={results['ks_pvalue']:.4f})\")\n",
        "    \n",
        "    # Perturbation analysis\n",
        "    print(f\"\\n2. PERTURBATION PATTERN ANALYSIS:\")\n",
        "    print(f\"   - Common perturbations: {len(pert_analysis['common_perts'])}\")\n",
        "    print(f\"   - Test-only perturbations: {len(pert_analysis['test_only'])}\")\n",
        "    print(f\"   - Train-only perturbations: {len(pert_analysis['train_only'])}\")\n",
        "    \n",
        "    if len(pert_analysis['test_only']) > 0:\n",
        "        print(f\"   - *** OUT-OF-DISTRIBUTION TESTING: {len(pert_analysis['test_only'])} perturbations not seen in training ***\")\n",
        "    \n",
        "    # Distance analysis\n",
        "    print(f\"\\n3. DISTANCE-BASED ANALYSIS:\")\n",
        "    train_within_mean = np.mean(distances['train_within'])\n",
        "    test_within_mean = np.mean(distances['test_within'])\n",
        "    between_mean = np.mean(distances['between'])\n",
        "    \n",
        "    print(f\"   - Train within-cluster distance: {train_within_mean:.4f}\")\n",
        "    print(f\"   - Test within-cluster distance: {test_within_mean:.4f}\")\n",
        "    print(f\"   - Train-Test between-cluster distance: {between_mean:.4f}\")\n",
        "    \n",
        "    if between_mean > max(train_within_mean, test_within_mean) * 1.2:\n",
        "        print(f\"   - *** SIGNIFICANT SEPARATION: Train and test clusters are well-separated ***\")\n",
        "    \n",
        "    # Overall conclusion\n",
        "    print(f\"\\n4. OVERALL ASSESSMENT:\")\n",
        "    ood_indicators = []\n",
        "    \n",
        "    if len(significant_diffs) > 0:\n",
        "        ood_indicators.append(f\"Gene expression differences ({len(significant_diffs)} metrics)\")\n",
        "    \n",
        "    if len(pert_analysis['test_only']) > 0:\n",
        "        ood_indicators.append(f\"Novel perturbations in test set ({len(pert_analysis['test_only'])} perturbations)\")\n",
        "    \n",
        "    if between_mean > max(train_within_mean, test_within_mean) * 1.2:\n",
        "        ood_indicators.append(\"Well-separated train/test clusters\")\n",
        "    \n",
        "    if ood_indicators:\n",
        "        print(f\"   *** EVIDENCE OF OUT-OF-DISTRIBUTION TESTING:\")\n",
        "        for indicator in ood_indicators:\n",
        "            print(f\"      - {indicator}\")\n",
        "        print(f\"\\n   *** CONCLUSION: Fine-tuning performance issues likely due to OOD testing ***\")\n",
        "    else:\n",
        "        print(f\"   *** CONCLUSION: Limited evidence of OOD issues - other factors may be responsible ***\")\n",
        "    \n",
        "    return {\n",
        "        'significant_diffs': significant_diffs,\n",
        "        'ood_indicators': ood_indicators,\n",
        "        'is_ood': len(ood_indicators) > 0\n",
        "    }\n",
        "\n",
        "# Generate summary\n",
        "summary = generate_summary_report(statistical_results, pert_analysis, distances)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save results for further analysis\n",
        "print(\"\\n=== SAVING RESULTS ===\")\n",
        "\n",
        "# Create results directory\n",
        "results_dir = Path(\"./distribution_analysis_results\")\n",
        "results_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# Save statistical results\n",
        "import json\n",
        "with open(results_dir / \"statistical_results.json\", \"w\") as f:\n",
        "    # Convert numpy types to Python types for JSON serialization\n",
        "    json_results = {}\n",
        "    for metric, results in statistical_results.items():\n",
        "        json_results[metric] = {k: float(v) if isinstance(v, (np.integer, np.floating)) else v \n",
        "                               for k, v in results.items()}\n",
        "    json.dump(json_results, f, indent=2)\n",
        "\n",
        "# Save perturbation analysis\n",
        "pert_json = {\n",
        "    'common_perts': list(pert_analysis['common_perts']),\n",
        "    'train_only': list(pert_analysis['train_only']),\n",
        "    'test_only': list(pert_analysis['test_only']),\n",
        "    'train_single_count': pert_analysis['train_single_count'],\n",
        "    'train_combo_count': pert_analysis['train_combo_count'],\n",
        "    'test_single_count': pert_analysis['test_single_count'],\n",
        "    'test_combo_count': pert_analysis['test_combo_count']\n",
        "}\n",
        "with open(results_dir / \"perturbation_analysis.json\", \"w\") as f:\n",
        "    json.dump(pert_json, f, indent=2)\n",
        "\n",
        "# Save summary\n",
        "with open(results_dir / \"summary.json\", \"w\") as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "# Save plots\n",
        "plt.savefig(results_dir / \"distribution_analysis.png\", dpi=300, bbox_inches='tight')\n",
        "\n",
        "print(f\"Results saved to {results_dir}/\")\n",
        "print(f\"Files created:\")\n",
        "print(f\"  - statistical_results.json\")\n",
        "print(f\"  - perturbation_analysis.json\") \n",
        "print(f\"  - summary.json\")\n",
        "print(f\"  - distribution_analysis.png\")\n",
        "\n",
        "print(f\"\\nAnalysis complete! Check the results directory for detailed outputs.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
