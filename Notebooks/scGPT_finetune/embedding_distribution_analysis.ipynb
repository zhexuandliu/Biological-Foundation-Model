{"cells":[{"cell_type":"markdown","metadata":{"id":"lYvYh5Ryw506"},"source":["# Forward Pass Embedding Distribution Analysis for scGPT Fine-tuning\n","\n","This notebook analyzes the distribution differences in forward pass embeddings between training and testing data to investigate potential representation shifts that might affect model generalization.\n","\n","## Overview\n","- **Goal**: Analyze if there are distribution differences in model embeddings between train/test data\n","- **Models**: Compare pretrained vs finetuned scGPT models\n","- **Analysis**: Statistical and visual comparison of embedding patterns across different model layers\n","- **Focus**: Understanding if fine-tuning changes how the model represents train vs test data differently\n"]},{"cell_type":"code","source":["from google.colab import drive\n","import os\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","os.chdir('/content/drive/MyDrive/GitHub/Biological-Foundation-Model/Notebooks/scGPT_finetune')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7u5aUMLT0OYh","executionInfo":{"status":"ok","timestamp":1759708864080,"user_tz":300,"elapsed":531,"user":{"displayName":"Zhexuan Liu","userId":"18073944608501350772"}},"outputId":"1e90da29-4ca1-4f31-f0ec-4da71ee786ab"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wQz2olbpw50-"},"outputs":[],"source":["# Setup and Installation for Google Colab\n","from google.colab import drive\n","import os\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","os.chdir('/content/drive/MyDrive/GitHub/Biological-Foundation-Model/notebooks/scGPT_finetune')\n","\n","# Install required packages\n","%pip install -r ./requirements.txt\n","%pip install scgpt \"flash-attn<1.0.5\"\n","%pip install seaborn plotly scipy umap-learn\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hVQTIdGLw50_"},"outputs":[],"source":["# Import libraries\n","import json\n","import os\n","import sys\n","import time\n","import copy\n","from pathlib import Path\n","from typing import Iterable, List, Tuple, Dict, Union, Optional\n","import warnings\n","\n","import torch\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import plotly.express as px\n","import plotly.graph_objects as go\n","from plotly.subplots import make_subplots\n","from scipy import stats\n","from scipy.spatial.distance import pdist, squareform\n","from sklearn.decomposition import PCA\n","from sklearn.manifold import TSNE\n","from sklearn.metrics import pairwise_distances\n","import umap\n","from torch import nn\n","from torch.nn import functional as F\n","from torchtext.vocab import Vocab\n","from torchtext._torchtext import (\n","    Vocab as VocabPybind,\n",")\n","from torch_geometric.loader import DataLoader\n","from gears import PertData, GEARS\n","from gears.inference import compute_metrics, deeper_analysis, non_dropout_analysis\n","from gears.utils import create_cell_graph_dataset_for_prediction\n","\n","sys.path.insert(0, \"../\")\n","\n","import scgpt as scg\n","from scgpt.model import TransformerGenerator\n","from scgpt.loss import (\n","    masked_mse_loss,\n","    criterion_neg_log_bernoulli,\n","    masked_relative_error,\n",")\n","from scgpt.tokenizer import tokenize_batch, pad_batch, tokenize_and_pad_batch\n","from scgpt.tokenizer.gene_tokenizer import GeneVocab\n","from scgpt.utils import set_seed, map_raw_id_to_vocab_id, compute_perturbation_metrics\n","\n","# Set up plotting\n","plt.style.use('default')\n","sns.set_palette(\"husl\")\n","matplotlib.rcParams[\"savefig.transparent\"] = False\n","warnings.filterwarnings(\"ignore\")\n","\n","set_seed(42)\n","print(\"Libraries imported successfully!\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UhEGnz50w51A"},"outputs":[],"source":["# Load and prepare data\n","print(\"Loading perturbation data...\")\n","\n","# Settings for data processing\n","pad_token = \"<pad>\"\n","special_tokens = [pad_token, \"<cls>\", \"<eoc>\"]\n","pad_value = 0\n","pert_pad_id = 0\n","include_zero_gene = \"all\"\n","max_seq_len = 1536\n","\n","# Dataset settings\n","data_name = \"adamson\"\n","split = \"simulation\"\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Load perturbation data\n","pert_data = PertData(\"./data\")\n","pert_data.load(data_name=data_name)\n","pert_data.prepare_split(split=split, seed=1)\n","pert_data.get_dataloader(batch_size=64, test_batch_size=64)\n","\n","print(f\"Data loaded successfully!\")\n","print(f\"Dataset: {data_name}\")\n","print(f\"Split: {split}\")\n","print(f\"Device: {device}\")\n","\n","# Get basic info about the dataset\n","adata = pert_data.adata\n","print(f\"\\nDataset info:\")\n","print(f\"Total cells: {adata.n_obs}\")\n","print(f\"Total genes: {adata.n_vars}\")\n","print(f\"Conditions: {adata.obs['condition'].unique()}\")\n","\n","# Extract train/test splits\n","def extract_split_data(adata, split_info, split_name):\n","    \"\"\"Extract data for a specific split\"\"\"\n","    if split_name == \"train\":\n","        split_cells = split_info[\"train_idx\"]\n","    elif split_name == \"test\":\n","        split_cells = split_info[\"test_idx\"]\n","    elif split_name == \"val\":\n","        split_cells = split_info[\"val_idx\"]\n","    else:\n","        raise ValueError(f\"Unknown split: {split_name}\")\n","\n","    return adata[split_cells].copy()\n","\n","train_adata = extract_split_data(adata, pert_data.split, \"train\")\n","test_adata = extract_split_data(adata, pert_data.split, \"test\")\n","val_adata = extract_split_data(adata, pert_data.split, \"val\")\n","\n","print(f\"\\nSplit sizes:\")\n","print(f\"Train: {train_adata.n_obs} cells\")\n","print(f\"Test: {test_adata.n_obs} cells\")\n","print(f\"Val: {val_adata.n_obs} cells\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"esxw8R26w51A"},"outputs":[],"source":["# Load pretrained and finetuned models\n","print(\"Loading models...\")\n","\n","# Model settings\n","load_model = \"./save/scGPT_human\"\n","load_param_prefixs = [\n","    \"encoder\",\n","    \"value_encoder\",\n","    \"transformer_encoder\",\n","]\n","\n","# Load model configuration\n","model_dir = Path(\"./save/scGPT_human\")\n","model_config_file = model_dir / \"args.json\"\n","model_file = model_dir / \"best_model.pt\"\n","vocab_file = model_dir / \"vocab.json\"\n","\n","vocab = GeneVocab.from_file(vocab_file)\n","for s in special_tokens:\n","    if s not in vocab:\n","        vocab.append_token(s)\n","\n","pert_data.adata.var[\"id_in_vocab\"] = [\n","    1 if gene in vocab else -1 for gene in pert_data.adata.var[\"gene_name\"]\n","]\n","gene_ids_in_vocab = np.array(pert_data.adata.var[\"id_in_vocab\"])\n","genes = pert_data.adata.var[\"gene_name\"].tolist()\n","\n","# Load model configuration\n","with open(model_config_file, \"r\") as f:\n","    model_configs = json.load(f)\n","\n","embsize = model_configs[\"embsize\"]\n","nhead = model_configs[\"nheads\"]\n","d_hid = model_configs[\"d_hid\"]\n","nlayers = model_configs[\"nlayers\"]\n","n_layers_cls = model_configs[\"n_layers_cls\"]\n","\n","vocab.set_default_index(vocab[\"<pad>\"])\n","gene_ids = np.array(\n","    [vocab[gene] if gene in vocab else vocab[\"<pad>\"] for gene in genes], dtype=int\n",")\n","n_genes = len(genes)\n","ntokens = len(vocab)\n","\n","print(f\"Model configuration loaded:\")\n","print(f\"  Vocabulary size: {ntokens}\")\n","print(f\"  Embedding size: {embsize}\")\n","print(f\"  Number of layers: {nlayers}\")\n","print(f\"  Genes in vocab: {np.sum(gene_ids_in_vocab >= 0)}/{len(gene_ids_in_vocab)}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xUtsMv1Zw51B"},"outputs":[],"source":["# Create and load pretrained model\n","print(\"Loading pretrained model...\")\n","model_pretrain = TransformerGenerator(\n","    ntokens,\n","    embsize,\n","    nhead,\n","    d_hid,\n","    nlayers,\n","    nlayers_cls=n_layers_cls,\n","    n_cls=1,\n","    vocab=vocab,\n","    dropout=0,\n","    pad_token=pad_token,\n","    pad_value=pad_value,\n","    pert_pad_id=pert_pad_id,\n","    use_fast_transformer=True,\n",")\n","\n","# Load pretrained weights\n","model_dict = model_pretrain.state_dict()\n","pretrained_dict = torch.load(model_file)\n","pretrained_dict = {\n","    k: v for k, v in pretrained_dict.items()\n","    if any([k.startswith(prefix) for prefix in load_param_prefixs])\n","}\n","for k, v in pretrained_dict.items():\n","    print(f\"Loading pretrained param {k} with shape {v.shape}\")\n","model_dict.update(pretrained_dict)\n","model_pretrain.load_state_dict(model_dict)\n","model_pretrain.to(device)\n","model_pretrain.eval()\n","\n","print(\"Pretrained model loaded successfully!\")\n","\n","# Load finetuned model\n","print(\"Loading finetuned model...\")\n","model_finetune = TransformerGenerator(\n","    ntokens,\n","    embsize,\n","    nhead,\n","    d_hid,\n","    nlayers,\n","    nlayers_cls=n_layers_cls,\n","    n_cls=1,\n","    vocab=vocab,\n","    dropout=0,\n","    pad_token=pad_token,\n","    pad_value=pad_value,\n","    pert_pad_id=pert_pad_id,\n","    use_fast_transformer=True,\n",")\n","\n","# Try to load finetuned weights\n","finetuned_model_dir = Path(\"./save/scGPT_human_finetuned_adamson\")\n","finetuned_model_file = finetuned_model_dir / \"best_model.pt\"\n","\n","if finetuned_model_file.exists():\n","    try:\n","        model_finetune.load_state_dict(torch.load(finetuned_model_file))\n","        print(\"Finetuned model loaded successfully!\")\n","    except Exception as e:\n","        print(f\"Error loading finetuned model: {e}\")\n","        print(\"Using pretrained model for both comparisons...\")\n","        model_finetune = copy.deepcopy(model_pretrain)\n","else:\n","    print(\"Finetuned model not found. Using pretrained model for both comparisons...\")\n","    model_finetune = copy.deepcopy(model_pretrain)\n","\n","model_finetune.to(device)\n","model_finetune.eval()\n","\n","print(\"Models ready for embedding extraction!\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ywSj0AM5w51B"},"outputs":[],"source":["# Define embedding extraction functions\n","def extract_embeddings_from_model(model, adata, split_name, max_cells=1000, layer_indices=None):\n","    \"\"\"\n","    Extract embeddings from different layers of the model\n","\n","    Args:\n","        model: The scGPT model\n","        adata: AnnData object with cell data\n","        split_name: Name of the split for logging\n","        max_cells: Maximum number of cells to process (for memory efficiency)\n","        layer_indices: List of layer indices to extract embeddings from (None for all layers)\n","\n","    Returns:\n","        Dictionary with embeddings from different layers\n","    \"\"\"\n","    model.eval()\n","    device = next(model.parameters()).device\n","\n","    # Sample cells if needed\n","    if adata.n_obs > max_cells:\n","        indices = np.random.choice(adata.n_obs, max_cells, replace=False)\n","        adata_sample = adata[indices].copy()\n","    else:\n","        adata_sample = adata.copy()\n","\n","    print(f\"Extracting embeddings from {adata_sample.n_obs} cells for {split_name}...\")\n","\n","    # Convert to dense if sparse\n","    if hasattr(adata_sample.X, 'toarray'):\n","        X = adata_sample.X.toarray()\n","    else:\n","        X = adata_sample.X\n","\n","    embeddings = {}\n","\n","    with torch.no_grad():\n","        # Process in batches\n","        batch_size = 32\n","        n_batches = (len(X) + batch_size - 1) // batch_size\n","\n","        all_embeddings = []\n","\n","        for batch_idx in range(n_batches):\n","            start_idx = batch_idx * batch_size\n","            end_idx = min((batch_idx + 1) * batch_size, len(X))\n","            batch_X = X[start_idx:end_idx]\n","\n","            # Prepare input\n","            input_gene_ids = torch.arange(n_genes, device=device, dtype=torch.long)\n","            mapped_input_gene_ids = map_raw_id_to_vocab_id(input_gene_ids, gene_ids)\n","            mapped_input_gene_ids = mapped_input_gene_ids.unsqueeze(0).repeat(len(batch_X), 1)\n","\n","            input_values = torch.from_numpy(batch_X).to(device=device, dtype=torch.float32)\n","            input_pert_flags = torch.zeros(len(batch_X), n_genes, dtype=torch.long, device=device)\n","            src_key_padding_mask = torch.zeros_like(input_values, dtype=torch.bool, device=device)\n","\n","            # Forward pass to get embeddings\n","            try:\n","                # Get embeddings from different layers\n","                tr_out = model._encode(\n","                    mapped_input_gene_ids,\n","                    input_values,\n","                    input_pert_flags,\n","                    src_key_padding_mask,\n","                )\n","\n","                # Extract cell embeddings (mean pooling over genes)\n","                cell_emb = model._get_cell_emb_from_layer(tr_out, input_values)\n","                all_embeddings.append(cell_emb.cpu().numpy())\n","\n","            except Exception as e:\n","                print(f\"Error processing batch {batch_idx}: {e}\")\n","                continue\n","\n","    if all_embeddings:\n","        embeddings['cell_embeddings'] = np.vstack(all_embeddings)\n","        print(f\"Extracted cell embeddings: {embeddings['cell_embeddings'].shape}\")\n","    else:\n","        print(f\"No embeddings extracted for {split_name}\")\n","        embeddings['cell_embeddings'] = np.array([])\n","\n","    return embeddings\n","\n","def extract_layer_wise_embeddings(model, adata, split_name, max_cells=500):\n","    \"\"\"\n","    Extract embeddings from different transformer layers\n","    \"\"\"\n","    model.eval()\n","    device = next(model.parameters()).device\n","\n","    # Sample cells if needed\n","    if adata.n_obs > max_cells:\n","        indices = np.random.choice(adata.n_obs, max_cells, replace=False)\n","        adata_sample = adata[indices].copy()\n","    else:\n","        adata_sample = adata.copy()\n","\n","    print(f\"Extracting layer-wise embeddings from {adata_sample.n_obs} cells for {split_name}...\")\n","\n","    # Convert to dense if sparse\n","    if hasattr(adata_sample.X, 'toarray'):\n","        X = adata_sample.X.toarray()\n","    else:\n","        X = adata_sample.X\n","\n","    layer_embeddings = {}\n","\n","    with torch.no_grad():\n","        # Process a smaller batch for layer-wise analysis\n","        batch_size = 16\n","        n_batches = min(4, (len(X) + batch_size - 1) // batch_size)  # Limit to 4 batches\n","\n","        for batch_idx in range(n_batches):\n","            start_idx = batch_idx * batch_size\n","            end_idx = min((batch_idx + 1) * batch_size, len(X))\n","            batch_X = X[start_idx:end_idx]\n","\n","            # Prepare input\n","            input_gene_ids = torch.arange(n_genes, device=device, dtype=torch.long)\n","            mapped_input_gene_ids = map_raw_id_to_vocab_id(input_gene_ids, gene_ids)\n","            mapped_input_gene_ids = mapped_input_gene_ids.unsqueeze(0).repeat(len(batch_X), 1)\n","\n","            input_values = torch.from_numpy(batch_X).to(device=device, dtype=torch.float32)\n","            input_pert_flags = torch.zeros(len(batch_X), n_genes, dtype=torch.long, device=device)\n","            src_key_padding_mask = torch.zeros_like(input_values, dtype=torch.bool, device=device)\n","\n","            try:\n","                # Get embeddings from encoder\n","                encoder_out = model.encoder(mapped_input_gene_ids, input_values, input_pert_flags, src_key_padding_mask)\n","\n","                # Extract from different transformer layers\n","                for layer_idx in range(min(6, len(model.transformer_encoder.layers))):  # First 6 layers\n","                    layer_output = model.transformer_encoder.layers[layer_idx](\n","                        encoder_out, src_key_padding_mask=src_key_padding_mask\n","                    )\n","\n","                    # Mean pool over genes to get cell-level embeddings\n","                    cell_emb = layer_output.mean(dim=1)  # [batch_size, hidden_dim]\n","\n","                    if f'layer_{layer_idx}' not in layer_embeddings:\n","                        layer_embeddings[f'layer_{layer_idx}'] = []\n","                    layer_embeddings[f'layer_{layer_idx}'].append(cell_emb.cpu().numpy())\n","\n","            except Exception as e:\n","                print(f\"Error processing batch {batch_idx} for layer analysis: {e}\")\n","                continue\n","\n","    # Concatenate all batches for each layer\n","    for layer_name in layer_embeddings:\n","        if layer_embeddings[layer_name]:\n","            layer_embeddings[layer_name] = np.vstack(layer_embeddings[layer_name])\n","            print(f\"Layer {layer_name} embeddings: {layer_embeddings[layer_name].shape}\")\n","\n","    return layer_embeddings\n","\n","print(\"Embedding extraction functions defined!\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iMFweGr0w51C"},"outputs":[],"source":["# Extract embeddings from pretrained model\n","print(\"=== EXTRACTING EMBEDDINGS FROM PRETRAINED MODEL ===\")\n","\n","# Extract cell embeddings\n","train_emb_pretrain = extract_embeddings_from_model(model_pretrain, train_adata, \"train\", max_cells=1000)\n","test_emb_pretrain = extract_embeddings_from_model(model_pretrain, test_adata, \"test\", max_cells=1000)\n","val_emb_pretrain = extract_embeddings_from_model(model_pretrain, val_adata, \"val\", max_cells=1000)\n","\n","# Extract layer-wise embeddings\n","print(\"\\nExtracting layer-wise embeddings from pretrained model...\")\n","train_layers_pretrain = extract_layer_wise_embeddings(model_pretrain, train_adata, \"train\", max_cells=500)\n","test_layers_pretrain = extract_layer_wise_embeddings(model_pretrain, test_adata, \"test\", max_cells=500)\n","\n","print(f\"\\nPretrained model embeddings extracted:\")\n","print(f\"  Train cell embeddings: {train_emb_pretrain['cell_embeddings'].shape}\")\n","print(f\"  Test cell embeddings: {test_emb_pretrain['cell_embeddings'].shape}\")\n","print(f\"  Val cell embeddings: {val_emb_pretrain['cell_embeddings'].shape}\")\n","print(f\"  Train layer embeddings: {len(train_layers_pretrain)} layers\")\n","print(f\"  Test layer embeddings: {len(test_layers_pretrain)} layers\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yc28HWdQw51C"},"outputs":[],"source":["# Extract embeddings from finetuned model\n","print(\"=== EXTRACTING EMBEDDINGS FROM FINETUNED MODEL ===\")\n","\n","# Extract cell embeddings\n","train_emb_finetune = extract_embeddings_from_model(model_finetune, train_adata, \"train\", max_cells=1000)\n","test_emb_finetune = extract_embeddings_from_model(model_finetune, test_adata, \"test\", max_cells=1000)\n","val_emb_finetune = extract_embeddings_from_model(model_finetune, val_adata, \"val\", max_cells=1000)\n","\n","# Extract layer-wise embeddings\n","print(\"\\nExtracting layer-wise embeddings from finetuned model...\")\n","train_layers_finetune = extract_layer_wise_embeddings(model_finetune, train_adata, \"train\", max_cells=500)\n","test_layers_finetune = extract_layer_wise_embeddings(model_finetune, test_adata, \"test\", max_cells=500)\n","\n","print(f\"\\nFinetuned model embeddings extracted:\")\n","print(f\"  Train cell embeddings: {train_emb_finetune['cell_embeddings'].shape}\")\n","print(f\"  Test cell embeddings: {test_emb_finetune['cell_embeddings'].shape}\")\n","print(f\"  Val cell embeddings: {val_emb_finetune['cell_embeddings'].shape}\")\n","print(f\"  Train layer embeddings: {len(train_layers_finetune)} layers\")\n","print(f\"  Test layer embeddings: {len(test_layers_finetune)} layers\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8hR-EZX1w51C"},"outputs":[],"source":["# Statistical analysis of embedding distributions\n","print(\"=== STATISTICAL ANALYSIS OF EMBEDDING DISTRIBUTIONS ===\")\n","\n","def analyze_embedding_distributions(train_emb, test_emb, model_name):\n","    \"\"\"Analyze distribution differences in embeddings\"\"\"\n","\n","    if train_emb['cell_embeddings'].size == 0 or test_emb['cell_embeddings'].size == 0:\n","        print(f\"No embeddings available for {model_name}\")\n","        return {}\n","\n","    train_embeddings = train_emb['cell_embeddings']\n","    test_embeddings = test_emb['cell_embeddings']\n","\n","    print(f\"\\nAnalyzing {model_name} embeddings:\")\n","    print(f\"  Train shape: {train_embeddings.shape}\")\n","    print(f\"  Test shape: {test_embeddings.shape}\")\n","\n","    # Calculate statistics\n","    train_mean = np.mean(train_embeddings, axis=0)\n","    test_mean = np.mean(test_embeddings, axis=0)\n","    train_std = np.std(train_embeddings, axis=0)\n","    test_std = np.std(test_embeddings, axis=0)\n","\n","    # Calculate per-cell statistics\n","    train_cell_norms = np.linalg.norm(train_embeddings, axis=1)\n","    test_cell_norms = np.linalg.norm(test_embeddings, axis=1)\n","    train_cell_means = np.mean(train_embeddings, axis=1)\n","    test_cell_means = np.mean(test_embeddings, axis=1)\n","\n","    # Statistical tests\n","    # 1. Mean embedding comparison (per dimension)\n","    mean_diffs = np.abs(train_mean - test_mean)\n","    mean_diff_pct = np.mean(mean_diffs) / np.mean(np.abs(train_mean)) * 100\n","\n","    # 2. Variance comparison (per dimension)\n","    var_diffs = np.abs(train_std - test_std)\n","    var_diff_pct = np.mean(var_diffs) / np.mean(train_std) * 100\n","\n","    # 3. Cell-level statistics\n","    ks_norm, ks_norm_p = stats.ks_2samp(train_cell_norms, test_cell_norms)\n","    ks_mean, ks_mean_p = stats.ks_2samp(train_cell_means, test_cell_means)\n","\n","    # 4. Cosine similarity between mean embeddings\n","    cosine_sim = np.dot(train_mean, test_mean) / (np.linalg.norm(train_mean) * np.linalg.norm(test_mean))\n","\n","    # 5. Centroid distance\n","    centroid_distance = np.linalg.norm(train_mean - test_mean)\n","\n","    results = {\n","        'mean_diff_pct': mean_diff_pct,\n","        'var_diff_pct': var_diff_pct,\n","        'ks_norm_stat': ks_norm,\n","        'ks_norm_pvalue': ks_norm_p,\n","        'ks_mean_stat': ks_mean,\n","        'ks_mean_pvalue': ks_mean_p,\n","        'cosine_similarity': cosine_sim,\n","        'centroid_distance': centroid_distance,\n","        'train_mean_norm': np.mean(train_cell_norms),\n","        'test_mean_norm': np.mean(test_cell_norms),\n","        'train_std_norm': np.std(train_cell_norms),\n","        'test_std_norm': np.std(test_cell_norms)\n","    }\n","\n","    print(f\"  Mean difference: {mean_diff_pct:.2f}%\")\n","    print(f\"  Variance difference: {var_diff_pct:.2f}%\")\n","    print(f\"  Cosine similarity: {cosine_sim:.4f}\")\n","    print(f\"  Centroid distance: {centroid_distance:.4f}\")\n","    print(f\"  KS test (norms): stat={ks_norm:.4f}, p={ks_norm_p:.4f}\")\n","    print(f\"  KS test (means): stat={ks_mean:.4f}, p={ks_mean_p:.4f}\")\n","\n","    if ks_norm_p < 0.05 or ks_mean_p < 0.05:\n","        print(f\"  *** SIGNIFICANT DIFFERENCE in embedding distributions ***\")\n","\n","    return results\n","\n","# Analyze pretrained model\n","pretrain_results = analyze_embedding_distributions(train_emb_pretrain, test_emb_pretrain, \"Pretrained\")\n","\n","# Analyze finetuned model\n","finetune_results = analyze_embedding_distributions(train_emb_finetune, test_emb_finetune, \"Finetuned\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lmpCptA4w51D"},"outputs":[],"source":["# Visualize embedding distributions\n","print(\"=== VISUALIZING EMBEDDING DISTRIBUTIONS ===\")\n","\n","def plot_embedding_distributions(train_emb, test_emb, model_name, max_cells=1000):\n","    \"\"\"Create visualizations of embedding distributions\"\"\"\n","\n","    if train_emb['cell_embeddings'].size == 0 or test_emb['cell_embeddings'].size == 0:\n","        print(f\"No embeddings available for {model_name} visualization\")\n","        return\n","\n","    train_embeddings = train_emb['cell_embeddings']\n","    test_embeddings = test_emb['cell_embeddings']\n","\n","    # Sample for visualization if too many cells\n","    if len(train_embeddings) > max_cells:\n","        train_indices = np.random.choice(len(train_embeddings), max_cells, replace=False)\n","        train_embeddings = train_embeddings[train_indices]\n","\n","    if len(test_embeddings) > max_cells:\n","        test_indices = np.random.choice(len(test_embeddings), max_cells, replace=False)\n","        test_embeddings = test_embeddings[test_indices]\n","\n","    # Calculate per-cell statistics\n","    train_norms = np.linalg.norm(train_embeddings, axis=1)\n","    test_norms = np.linalg.norm(test_embeddings, axis=1)\n","    train_means = np.mean(train_embeddings, axis=1)\n","    test_means = np.mean(test_embeddings, axis=1)\n","\n","    # Create subplots\n","    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n","    fig.suptitle(f'Embedding Distribution Analysis: {model_name}', fontsize=16)\n","\n","    # 1. Embedding norms distribution\n","    axes[0, 0].hist(train_norms, bins=50, alpha=0.7, label='Train', density=True)\n","    axes[0, 0].hist(test_norms, bins=50, alpha=0.7, label='Test', density=True)\n","    axes[0, 0].set_xlabel('Embedding Norm')\n","    axes[0, 0].set_ylabel('Density')\n","    axes[0, 0].set_title('Embedding Norm Distribution')\n","    axes[0, 0].legend()\n","\n","    # 2. Embedding means distribution\n","    axes[0, 1].hist(train_means, bins=50, alpha=0.7, label='Train', density=True)\n","    axes[0, 1].hist(test_means, bins=50, alpha=0.7, label='Test', density=True)\n","    axes[0, 1].set_xlabel('Mean Embedding Value')\n","    axes[0, 1].set_ylabel('Density')\n","    axes[0, 1].set_title('Mean Embedding Distribution')\n","    axes[0, 1].legend()\n","\n","    # 3. PCA visualization\n","    try:\n","        # Combine embeddings for PCA\n","        combined_embeddings = np.vstack([train_embeddings, test_embeddings])\n","        pca = PCA(n_components=2, random_state=42)\n","        pca_embeddings = pca.fit_transform(combined_embeddings)\n","\n","        train_pca = pca_embeddings[:len(train_embeddings)]\n","        test_pca = pca_embeddings[len(train_embeddings):]\n","\n","        axes[0, 2].scatter(train_pca[:, 0], train_pca[:, 1], alpha=0.6, label='Train', s=20)\n","        axes[0, 2].scatter(test_pca[:, 0], test_pca[:, 1], alpha=0.6, label='Test', s=20)\n","        axes[0, 2].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%})')\n","        axes[0, 2].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%})')\n","        axes[0, 2].set_title('PCA Visualization')\n","        axes[0, 2].legend()\n","    except Exception as e:\n","        axes[0, 2].text(0.5, 0.5, f'PCA Error: {str(e)}', ha='center', va='center')\n","        axes[0, 2].set_title('PCA Visualization (Error)')\n","\n","    # 4. Box plot comparison\n","    data_to_plot = [train_norms, test_norms]\n","    axes[1, 0].boxplot(data_to_plot, labels=['Train', 'Test'])\n","    axes[1, 0].set_ylabel('Embedding Norm')\n","    axes[1, 0].set_title('Embedding Norm Comparison')\n","\n","    # 5. UMAP visualization (if available)\n","    try:\n","        import umap\n","        reducer = umap.UMAP(n_components=2, random_state=42, n_neighbors=15, min_dist=0.1)\n","        umap_embeddings = reducer.fit_transform(combined_embeddings)\n","\n","        train_umap = umap_embeddings[:len(train_embeddings)]\n","        test_umap = umap_embeddings[len(train_embeddings):]\n","\n","        axes[1, 1].scatter(train_umap[:, 0], train_umap[:, 1], alpha=0.6, label='Train', s=20)\n","        axes[1, 1].scatter(test_umap[:, 0], test_umap[:, 1], alpha=0.6, label='Test', s=20)\n","        axes[1, 1].set_xlabel('UMAP 1')\n","        axes[1, 1].set_ylabel('UMAP 2')\n","        axes[1, 1].set_title('UMAP Visualization')\n","        axes[1, 1].legend()\n","    except Exception as e:\n","        axes[1, 1].text(0.5, 0.5, f'UMAP Error: {str(e)}', ha='center', va='center')\n","        axes[1, 1].set_title('UMAP Visualization (Error)')\n","\n","    # 6. Distance analysis\n","    try:\n","        # Calculate pairwise distances\n","        train_distances = pairwise_distances(train_embeddings, metric='cosine')\n","        test_distances = pairwise_distances(test_embeddings, metric='cosine')\n","        between_distances = pairwise_distances(train_embeddings, test_embeddings, metric='cosine')\n","\n","        train_within = train_distances[np.triu_indices_from(train_distances, k=1)]\n","        test_within = test_distances[np.triu_indices_from(test_distances, k=1)]\n","        between = between_distances.flatten()\n","\n","        axes[1, 2].hist(train_within, bins=30, alpha=0.7, label='Train within', density=True)\n","        axes[1, 2].hist(test_within, bins=30, alpha=0.7, label='Test within', density=True)\n","        axes[1, 2].hist(between, bins=30, alpha=0.7, label='Train-Test between', density=True)\n","        axes[1, 2].set_xlabel('Cosine Distance')\n","        axes[1, 2].set_ylabel('Density')\n","        axes[1, 2].set_title('Distance Distributions')\n","        axes[1, 2].legend()\n","    except Exception as e:\n","        axes[1, 2].text(0.5, 0.5, f'Distance Error: {str(e)}', ha='center', va='center')\n","        axes[1, 2].set_title('Distance Analysis (Error)')\n","\n","    plt.tight_layout()\n","    plt.show()\n","\n","# Visualize pretrained model\n","plot_embedding_distributions(train_emb_pretrain, test_emb_pretrain, \"Pretrained Model\")\n","\n","# Visualize finetuned model\n","plot_embedding_distributions(train_emb_finetune, test_emb_finetune, \"Finetuned Model\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uJycasRtw51D"},"outputs":[],"source":["# Layer-wise analysis\n","print(\"=== LAYER-WISE EMBEDDING ANALYSIS ===\")\n","\n","def analyze_layer_wise_distributions(train_layers, test_layers, model_name):\n","    \"\"\"Analyze distribution differences across transformer layers\"\"\"\n","\n","    if not train_layers or not test_layers:\n","        print(f\"No layer-wise embeddings available for {model_name}\")\n","        return {}\n","\n","    layer_results = {}\n","\n","    for layer_name in train_layers:\n","        if layer_name not in test_layers:\n","            continue\n","\n","        train_layer_emb = train_layers[layer_name]\n","        test_layer_emb = test_layers[layer_name]\n","\n","        if train_layer_emb.size == 0 or test_layer_emb.size == 0:\n","            continue\n","\n","        print(f\"\\nAnalyzing {layer_name} for {model_name}:\")\n","        print(f\"  Train shape: {train_layer_emb.shape}\")\n","        print(f\"  Test shape: {test_layer_emb.shape}\")\n","\n","        # Calculate statistics\n","        train_norms = np.linalg.norm(train_layer_emb, axis=1)\n","        test_norms = np.linalg.norm(test_layer_emb, axis=1)\n","        train_means = np.mean(train_layer_emb, axis=1)\n","        test_means = np.mean(test_layer_emb, axis=1)\n","\n","        # Statistical tests\n","        ks_norm, ks_norm_p = stats.ks_2samp(train_norms, test_norms)\n","        ks_mean, ks_mean_p = stats.ks_2samp(train_means, test_means)\n","\n","        # Centroid distance\n","        train_centroid = np.mean(train_layer_emb, axis=0)\n","        test_centroid = np.mean(test_layer_emb, axis=0)\n","        centroid_distance = np.linalg.norm(train_centroid - test_centroid)\n","\n","        # Cosine similarity\n","        cosine_sim = np.dot(train_centroid, test_centroid) / (np.linalg.norm(train_centroid) * np.linalg.norm(test_centroid))\n","\n","        layer_results[layer_name] = {\n","            'ks_norm_stat': ks_norm,\n","            'ks_norm_pvalue': ks_norm_p,\n","            'ks_mean_stat': ks_mean,\n","            'ks_mean_pvalue': ks_mean_p,\n","            'centroid_distance': centroid_distance,\n","            'cosine_similarity': cosine_sim,\n","            'train_mean_norm': np.mean(train_norms),\n","            'test_mean_norm': np.mean(test_norms)\n","        }\n","\n","        print(f\"  Centroid distance: {centroid_distance:.4f}\")\n","        print(f\"  Cosine similarity: {cosine_sim:.4f}\")\n","        print(f\"  KS test (norms): stat={ks_norm:.4f}, p={ks_norm_p:.4f}\")\n","        print(f\"  KS test (means): stat={ks_mean:.4f}, p={ks_mean_p:.4f}\")\n","\n","        if ks_norm_p < 0.05 or ks_mean_p < 0.05:\n","            print(f\"  *** SIGNIFICANT DIFFERENCE in {layer_name} ***\")\n","\n","    return layer_results\n","\n","# Analyze pretrained model layers\n","pretrain_layer_results = analyze_layer_wise_distributions(train_layers_pretrain, test_layers_pretrain, \"Pretrained\")\n","\n","# Analyze finetuned model layers\n","finetune_layer_results = analyze_layer_wise_distributions(train_layers_finetune, test_layers_finetune, \"Finetuned\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kVGSbCJrw51D"},"outputs":[],"source":["# Summary and conclusions\n","print(\"=== EMBEDDING DISTRIBUTION ANALYSIS SUMMARY ===\")\n","\n","def generate_embedding_summary(pretrain_results, finetune_results, pretrain_layer_results, finetune_layer_results):\n","    \"\"\"Generate comprehensive summary of embedding analysis\"\"\"\n","\n","    print(\"EMBEDDING DISTRIBUTION ANALYSIS SUMMARY\")\n","    print(\"=\" * 60)\n","\n","    # Overall embedding analysis\n","    print(\"\\n1. OVERALL EMBEDDING DISTRIBUTION DIFFERENCES:\")\n","\n","    if pretrain_results:\n","        print(f\"\\n   PRETRAINED MODEL:\")\n","        print(f\"   - Mean difference: {pretrain_results.get('mean_diff_pct', 0):.2f}%\")\n","        print(f\"   - Cosine similarity: {pretrain_results.get('cosine_similarity', 0):.4f}\")\n","        print(f\"   - Centroid distance: {pretrain_results.get('centroid_distance', 0):.4f}\")\n","        if pretrain_results.get('ks_norm_pvalue', 1) < 0.05 or pretrain_results.get('ks_mean_pvalue', 1) < 0.05:\n","            print(f\"   - *** SIGNIFICANT DIFFERENCE in pretrained embeddings ***\")\n","        else:\n","            print(f\"   - No significant difference in pretrained embeddings\")\n","\n","    if finetune_results:\n","        print(f\"\\n   FINETUNED MODEL:\")\n","        print(f\"   - Mean difference: {finetune_results.get('mean_diff_pct', 0):.2f}%\")\n","        print(f\"   - Cosine similarity: {finetune_results.get('cosine_similarity', 0):.4f}\")\n","        print(f\"   - Centroid distance: {finetune_results.get('centroid_distance', 0):.4f}\")\n","        if finetune_results.get('ks_norm_pvalue', 1) < 0.05 or finetune_results.get('ks_mean_pvalue', 1) < 0.05:\n","            print(f\"   - *** SIGNIFICANT DIFFERENCE in finetuned embeddings ***\")\n","        else:\n","            print(f\"   - No significant difference in finetuned embeddings\")\n","\n","    # Layer-wise analysis\n","    print(f\"\\n2. LAYER-WISE DISTRIBUTION DIFFERENCES:\")\n","\n","    if pretrain_layer_results:\n","        print(f\"\\n   PRETRAINED MODEL LAYERS:\")\n","        significant_layers_pretrain = []\n","        for layer_name, results in pretrain_layer_results.items():\n","            if results.get('ks_norm_pvalue', 1) < 0.05 or results.get('ks_mean_pvalue', 1) < 0.05:\n","                significant_layers_pretrain.append(layer_name)\n","                print(f\"   - {layer_name}: SIGNIFICANT (p<0.05)\")\n","            else:\n","                print(f\"   - {layer_name}: Not significant\")\n","\n","    if finetune_layer_results:\n","        print(f\"\\n   FINETUNED MODEL LAYERS:\")\n","        significant_layers_finetune = []\n","        for layer_name, results in finetune_layer_results.items():\n","            if results.get('ks_norm_pvalue', 1) < 0.05 or results.get('ks_mean_pvalue', 1) < 0.05:\n","                significant_layers_finetune.append(layer_name)\n","                print(f\"   - {layer_name}: SIGNIFICANT (p<0.05)\")\n","            else:\n","                print(f\"   - {layer_name}: Not significant\")\n","\n","    # Model comparison\n","    print(f\"\\n3. PRETRAINED vs FINETUNED COMPARISON:\")\n","\n","    if pretrain_results and finetune_results:\n","        pretrain_diff = pretrain_results.get('mean_diff_pct', 0)\n","        finetune_diff = finetune_results.get('mean_diff_pct', 0)\n","\n","        if finetune_diff > pretrain_diff * 1.2:\n","            print(f\"   - *** FINETUNING INCREASED distribution differences ***\")\n","            print(f\"   - Pretrained difference: {pretrain_diff:.2f}%\")\n","            print(f\"   - Finetuned difference: {finetune_diff:.2f}%\")\n","        elif finetune_diff < pretrain_diff * 0.8:\n","            print(f\"   - Finetuning reduced distribution differences\")\n","            print(f\"   - Pretrained difference: {pretrain_diff:.2f}%\")\n","            print(f\"   - Finetuned difference: {finetune_diff:.2f}%\")\n","        else:\n","            print(f\"   - Finetuning had minimal effect on distribution differences\")\n","            print(f\"   - Pretrained difference: {pretrain_diff:.2f}%\")\n","            print(f\"   - Finetuned difference: {finetune_diff:.2f}%\")\n","\n","    # Overall conclusion\n","    print(f\"\\n4. OVERALL ASSESSMENT:\")\n","\n","    embedding_ood_indicators = []\n","\n","    if pretrain_results and (pretrain_results.get('ks_norm_pvalue', 1) < 0.05 or pretrain_results.get('ks_mean_pvalue', 1) < 0.05):\n","        embedding_ood_indicators.append(\"Pretrained model shows embedding distribution differences\")\n","\n","    if finetune_results and (finetune_results.get('ks_norm_pvalue', 1) < 0.05 or finetune_results.get('ks_mean_pvalue', 1) < 0.05):\n","        embedding_ood_indicators.append(\"Finetuned model shows embedding distribution differences\")\n","\n","    if pretrain_layer_results and any(results.get('ks_norm_pvalue', 1) < 0.05 or results.get('ks_mean_pvalue', 1) < 0.05 for results in pretrain_layer_results.values()):\n","        embedding_ood_indicators.append(\"Layer-wise differences in pretrained model\")\n","\n","    if finetune_layer_results and any(results.get('ks_norm_pvalue', 1) < 0.05 or results.get('ks_mean_pvalue', 1) < 0.05 for results in finetune_layer_results.values()):\n","        embedding_ood_indicators.append(\"Layer-wise differences in finetuned model\")\n","\n","    if embedding_ood_indicators:\n","        print(f\"   *** EVIDENCE OF EMBEDDING DISTRIBUTION DIFFERENCES:\")\n","        for indicator in embedding_ood_indicators:\n","            print(f\"      - {indicator}\")\n","        print(f\"\\n   *** CONCLUSION: Model representations differ between train/test data ***\")\n","        print(f\"   *** This may contribute to fine-tuning performance issues ***\")\n","    else:\n","        print(f\"   *** CONCLUSION: Limited evidence of embedding distribution differences ***\")\n","        print(f\"   *** Embedding differences may not be the primary cause of performance issues ***\")\n","\n","    return {\n","        'embedding_ood_indicators': embedding_ood_indicators,\n","        'has_embedding_ood': len(embedding_ood_indicators) > 0\n","    }\n","\n","# Generate summary\n","embedding_summary = generate_embedding_summary(\n","    pretrain_results, finetune_results,\n","    pretrain_layer_results, finetune_layer_results\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iNUZRIo9w51E"},"outputs":[],"source":["# Save results\n","print(\"\\n=== SAVING RESULTS ===\")\n","\n","# Create results directory\n","results_dir = Path(\"./embedding_analysis_results\")\n","results_dir.mkdir(exist_ok=True)\n","\n","# Save statistical results\n","with open(results_dir / \"embedding_statistical_results.json\", \"w\") as f:\n","    json_results = {\n","        'pretrain_results': {k: float(v) if isinstance(v, (np.integer, np.floating)) else v\n","                           for k, v in pretrain_results.items()},\n","        'finetune_results': {k: float(v) if isinstance(v, (np.integer, np.floating)) else v\n","                           for k, v in finetune_results.items()}\n","    }\n","    json.dump(json_results, f, indent=2)\n","\n","# Save layer-wise results\n","with open(results_dir / \"layer_wise_results.json\", \"w\") as f:\n","    layer_json = {\n","        'pretrain_layers': {k: {kk: float(vv) if isinstance(vv, (np.integer, np.floating)) else vv\n","                              for kk, vv in v.items()}\n","                           for k, v in pretrain_layer_results.items()},\n","        'finetune_layers': {k: {kk: float(vv) if isinstance(vv, (np.integer, np.floating)) else vv\n","                              for kk, vv in v.items()}\n","                           for k, v in finetune_layer_results.items()}\n","    }\n","    json.dump(layer_json, f, indent=2)\n","\n","# Save summary\n","with open(results_dir / \"embedding_summary.json\", \"w\") as f:\n","    json.dump(embedding_summary, f, indent=2)\n","\n","# Save embeddings for further analysis (sample only)\n","if train_emb_pretrain['cell_embeddings'].size > 0:\n","    np.save(results_dir / \"train_embeddings_pretrain.npy\", train_emb_pretrain['cell_embeddings'])\n","if test_emb_pretrain['cell_embeddings'].size > 0:\n","    np.save(results_dir / \"test_embeddings_pretrain.npy\", test_emb_pretrain['cell_embeddings'])\n","if train_emb_finetune['cell_embeddings'].size > 0:\n","    np.save(results_dir / \"train_embeddings_finetune.npy\", train_emb_finetune['cell_embeddings'])\n","if test_emb_finetune['cell_embeddings'].size > 0:\n","    np.save(results_dir / \"test_embeddings_finetune.npy\", test_emb_finetune['cell_embeddings'])\n","\n","print(f\"Results saved to {results_dir}/\")\n","print(f\"Files created:\")\n","print(f\"  - embedding_statistical_results.json\")\n","print(f\"  - layer_wise_results.json\")\n","print(f\"  - embedding_summary.json\")\n","print(f\"  - train/test embeddings (numpy files)\")\n","\n","print(f\"\\nEmbedding distribution analysis complete!\")\n","print(f\"Check the results directory for detailed outputs and further analysis.\")\n"]}],"metadata":{"language_info":{"name":"python"},"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":0}