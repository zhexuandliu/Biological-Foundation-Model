{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Performance Comparison: Pre-trained vs Fine-tuned scGPT\n",
        "\n",
        "This notebook provides a comprehensive comparison of the pre-trained and fine-tuned scGPT models' performance on both training and test data to assess the effectiveness of fine-tuning.\n",
        "\n",
        "## Overview\n",
        "- **Goal**: Compare performance between pre-trained and fine-tuned models on training and test data\n",
        "- **Dataset**: Adamson perturbation data with simulation split\n",
        "- **Analysis**: Multiple evaluation metrics including perturbation prediction accuracy, gene expression reconstruction, and downstream task performance\n",
        "- **Context**: Previous analysis showed OOD issues - this notebook quantifies how well fine-tuning addresses them\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import json\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import copy\n",
        "from pathlib import Path\n",
        "from typing import Iterable, List, Tuple, Dict, Union, Optional\n",
        "import warnings\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "from scipy import stats\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.decomposition import PCA\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torchtext.vocab import Vocab\n",
        "from torchtext._torchtext import Vocab as VocabPybind\n",
        "from torch_geometric.loader import DataLoader\n",
        "from gears import PertData, GEARS\n",
        "from gears.inference import compute_metrics, deeper_analysis, non_dropout_analysis\n",
        "from gears.utils import create_cell_graph_dataset_for_prediction\n",
        "\n",
        "sys.path.insert(0, \"../\")\n",
        "\n",
        "import scgpt as scg\n",
        "from scgpt.model import TransformerGenerator\n",
        "from scgpt.loss import (\n",
        "    masked_mse_loss,\n",
        "    criterion_neg_log_bernoulli,\n",
        "    masked_relative_error,\n",
        ")\n",
        "from scgpt.tokenizer import tokenize_batch, pad_batch, tokenize_and_pad_batch\n",
        "from scgpt.tokenizer.gene_tokenizer import GeneVocab\n",
        "from scgpt.utils import set_seed, map_raw_id_to_vocab_id, compute_perturbation_metrics\n",
        "\n",
        "# Set up plotting\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "set_seed(42)\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and prepare data\n",
        "print(\"Loading perturbation data...\")\n",
        "\n",
        "# Settings for data processing\n",
        "pad_token = \"<pad>\"\n",
        "special_tokens = [pad_token, \"<cls>\", \"<eoc>\"]\n",
        "pad_value = 0\n",
        "pert_pad_id = 0\n",
        "include_zero_gene = \"all\"\n",
        "max_seq_len = 1536\n",
        "\n",
        "# Dataset settings\n",
        "data_name = \"adamson\"\n",
        "split = \"simulation\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load perturbation data\n",
        "pert_data = PertData(\"./data\")\n",
        "pert_data.load(data_name=data_name)\n",
        "pert_data.prepare_split(split=split, seed=1)\n",
        "pert_data.get_dataloader(batch_size=64, test_batch_size=64)\n",
        "\n",
        "print(f\"Data loaded successfully!\")\n",
        "print(f\"Dataset: {data_name}\")\n",
        "print(f\"Split: {split}\")\n",
        "print(f\"Device: {device}\")\n",
        "\n",
        "# Get basic info about the dataset\n",
        "adata = pert_data.adata\n",
        "print(f\"\\nDataset info:\")\n",
        "print(f\"Total cells: {adata.n_obs}\")\n",
        "print(f\"Total genes: {adata.n_vars}\")\n",
        "print(f\"Conditions: {len(adata.obs['condition'].unique())} unique conditions\")\n",
        "\n",
        "# Extract train/test splits\n",
        "def extract_split_data_by_conditions(adata, set2conditions, split_name):\n",
        "    \"\"\"Extract data for a specific split based on conditions\"\"\"\n",
        "    if split_name not in set2conditions:\n",
        "        raise ValueError(f\"Unknown split: {split_name}\")\n",
        "\n",
        "    # Get conditions for this split\n",
        "    split_conditions = set2conditions[split_name]\n",
        "\n",
        "    # Create boolean mask for cells in this split\n",
        "    split_mask = adata.obs['condition'].isin(split_conditions)\n",
        "\n",
        "    return adata[split_mask].copy()\n",
        "\n",
        "train_adata = extract_split_data_by_conditions(adata, pert_data.set2conditions, \"train\")\n",
        "test_adata = extract_split_data_by_conditions(adata, pert_data.set2conditions, \"test\")\n",
        "val_adata = extract_split_data_by_conditions(adata, pert_data.set2conditions, \"val\")\n",
        "\n",
        "print(f\"\\nSplit sizes:\")\n",
        "print(f\"Train: {train_adata.n_obs} cells\")\n",
        "print(f\"Test: {test_adata.n_obs} cells\")\n",
        "print(f\"Val: {val_adata.n_obs} cells\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load pretrained and finetuned models\n",
        "print(\"Loading models...\")\n",
        "\n",
        "# Model settings\n",
        "load_model = \"./save/scGPT_human\"\n",
        "load_param_prefixs = [\n",
        "    \"encoder\",\n",
        "    \"value_encoder\", \n",
        "    \"transformer_encoder\",\n",
        "]\n",
        "\n",
        "# Load model configuration\n",
        "model_dir = Path(\"./save/scGPT_human\")\n",
        "model_config_file = model_dir / \"args.json\"\n",
        "model_file = model_dir / \"best_model.pt\"\n",
        "vocab_file = model_dir / \"vocab.json\"\n",
        "\n",
        "vocab = GeneVocab.from_file(vocab_file)\n",
        "for s in special_tokens:\n",
        "    if s not in vocab:\n",
        "        vocab.append_token(s)\n",
        "\n",
        "pert_data.adata.var[\"id_in_vocab\"] = [\n",
        "    1 if gene in vocab else -1 for gene in pert_data.adata.var[\"gene_name\"]\n",
        "]\n",
        "gene_ids_in_vocab = np.array(pert_data.adata.var[\"id_in_vocab\"])\n",
        "genes = pert_data.adata.var[\"gene_name\"].tolist()\n",
        "\n",
        "# Load model configuration\n",
        "with open(model_config_file, \"r\") as f:\n",
        "    model_configs = json.load(f)\n",
        "\n",
        "embsize = model_configs[\"embsize\"]\n",
        "nhead = model_configs[\"nheads\"]\n",
        "d_hid = model_configs[\"d_hid\"]\n",
        "nlayers = model_configs[\"nlayers\"]\n",
        "n_layers_cls = model_configs[\"n_layers_cls\"]\n",
        "\n",
        "vocab.set_default_index(vocab[\"<pad>\"])\n",
        "gene_ids = np.array(\n",
        "    [vocab[gene] if gene in vocab else vocab[\"<pad>\"] for gene in genes], dtype=int\n",
        ")\n",
        "n_genes = len(genes)\n",
        "ntokens = len(vocab)\n",
        "\n",
        "print(f\"Model configuration loaded:\")\n",
        "print(f\"  Vocabulary size: {ntokens}\")\n",
        "print(f\"  Embedding size: {embsize}\")\n",
        "print(f\"  Number of layers: {nlayers}\")\n",
        "print(f\"  Genes in vocab: {np.sum(gene_ids_in_vocab >= 0)}/{len(gene_ids_in_vocab)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create and load pretrained model\n",
        "print(\"Loading pretrained model...\")\n",
        "model_pretrain = TransformerGenerator(\n",
        "    ntokens,\n",
        "    embsize,\n",
        "    nhead,\n",
        "    d_hid,\n",
        "    nlayers,\n",
        "    nlayers_cls=n_layers_cls,\n",
        "    n_cls=1,\n",
        "    vocab=vocab,\n",
        "    dropout=0,\n",
        "    pad_token=pad_token,\n",
        "    pad_value=pad_value,\n",
        "    pert_pad_id=pert_pad_id,\n",
        "    use_fast_transformer=True,\n",
        ")\n",
        "\n",
        "# Load pretrained weights\n",
        "model_dict = model_pretrain.state_dict()\n",
        "pretrained_dict = torch.load(model_file)\n",
        "pretrained_dict = {\n",
        "    k: v for k, v in pretrained_dict.items()\n",
        "    if any([k.startswith(prefix) for prefix in load_param_prefixs])\n",
        "}\n",
        "for k, v in pretrained_dict.items():\n",
        "    print(f\"Loading pretrained param {k} with shape {v.shape}\")\n",
        "model_dict.update(pretrained_dict)\n",
        "model_pretrain.load_state_dict(model_dict)\n",
        "model_pretrain.to(device)\n",
        "model_pretrain.eval()\n",
        "\n",
        "print(\"Pretrained model loaded successfully!\")\n",
        "\n",
        "# Load finetuned model\n",
        "print(\"Loading finetuned model...\")\n",
        "model_finetune = TransformerGenerator(\n",
        "    ntokens,\n",
        "    embsize,\n",
        "    nhead,\n",
        "    d_hid,\n",
        "    nlayers,\n",
        "    nlayers_cls=n_layers_cls,\n",
        "    n_cls=1,\n",
        "    vocab=vocab,\n",
        "    dropout=0,\n",
        "    pad_token=pad_token,\n",
        "    pad_value=pad_value,\n",
        "    pert_pad_id=pert_pad_id,\n",
        "    use_fast_transformer=True,\n",
        ")\n",
        "\n",
        "# Try to load finetuned weights\n",
        "finetuned_model_dir = Path(\"./save/scGPT_human_finetuned_adamson\")\n",
        "finetuned_model_file = finetuned_model_dir / \"best_model.pt\"\n",
        "\n",
        "if finetuned_model_file.exists():\n",
        "    try:\n",
        "        model_finetune.load_state_dict(torch.load(finetuned_model_file))\n",
        "        print(\"Finetuned model loaded successfully!\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading finetuned model: {e}\")\n",
        "        print(\"Using pretrained model for both comparisons...\")\n",
        "        model_finetune = copy.deepcopy(model_pretrain)\n",
        "else:\n",
        "    print(\"Finetuned model not found. Using pretrained model for both comparisons...\")\n",
        "    model_finetune = copy.deepcopy(model_pretrain)\n",
        "\n",
        "model_finetune.to(device)\n",
        "model_finetune.eval()\n",
        "\n",
        "print(\"Models ready for evaluation!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define evaluation functions\n",
        "def evaluate_model_performance(model, adata, split_name, max_cells=1000, batch_size=32):\n",
        "    \"\"\"\n",
        "    Evaluate model performance on a dataset split\n",
        "    \n",
        "    Args:\n",
        "        model: The scGPT model to evaluate\n",
        "        adata: AnnData object with cell data\n",
        "        split_name: Name of the split for logging\n",
        "        max_cells: Maximum number of cells to evaluate (for memory efficiency)\n",
        "        batch_size: Batch size for evaluation\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary with evaluation metrics\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "    \n",
        "    # Sample cells if needed\n",
        "    if adata.n_obs > max_cells:\n",
        "        indices = np.random.choice(adata.n_obs, max_cells, replace=False)\n",
        "        adata_sample = adata[indices].copy()\n",
        "    else:\n",
        "        adata_sample = adata.copy()\n",
        "    \n",
        "    print(f\"Evaluating {split_name} with {adata_sample.n_obs} cells...\")\n",
        "    \n",
        "    # Convert to dense if sparse\n",
        "    if hasattr(adata_sample.X, 'toarray'):\n",
        "        X = adata_sample.X.toarray()\n",
        "    else:\n",
        "        X = adata_sample.X\n",
        "    \n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "    all_losses = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        n_batches = (len(X) + batch_size - 1) // batch_size\n",
        "        \n",
        "        for batch_idx in range(n_batches):\n",
        "            start_idx = batch_idx * batch_size\n",
        "            end_idx = min((batch_idx + 1) * batch_size, len(X))\n",
        "            batch_X = X[start_idx:end_idx]\n",
        "            \n",
        "            # Prepare input\n",
        "            input_gene_ids = torch.arange(n_genes, device=device, dtype=torch.long)\n",
        "            mapped_input_gene_ids = map_raw_id_to_vocab_id(input_gene_ids, gene_ids)\n",
        "            mapped_input_gene_ids = mapped_input_gene_ids.unsqueeze(0).repeat(len(batch_X), 1)\n",
        "            \n",
        "            input_values = torch.from_numpy(batch_X).to(device=device, dtype=torch.float32)\n",
        "            input_pert_flags = torch.zeros(len(batch_X), n_genes, dtype=torch.long, device=device)\n",
        "            src_key_padding_mask = torch.zeros_like(input_values, dtype=torch.bool, device=device)\n",
        "            \n",
        "            try:\n",
        "                # Forward pass\n",
        "                output = model(\n",
        "                    mapped_input_gene_ids,\n",
        "                    input_values,\n",
        "                    input_pert_flags,\n",
        "                    src_key_padding_mask=src_key_padding_mask,\n",
        "                )\n",
        "                \n",
        "                # Calculate loss (MSE for reconstruction)\n",
        "                loss = F.mse_loss(output, input_values, reduction='none')\n",
        "                loss = loss.mean(dim=1)  # Average over genes per cell\n",
        "                \n",
        "                all_predictions.append(output.cpu().numpy())\n",
        "                all_targets.append(batch_X)\n",
        "                all_losses.append(loss.cpu().numpy())\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"Error in batch {batch_idx}: {e}\")\n",
        "                continue\n",
        "    \n",
        "    if not all_predictions:\n",
        "        print(f\"No valid predictions for {split_name}\")\n",
        "        return {}\n",
        "    \n",
        "    # Concatenate results\n",
        "    predictions = np.vstack(all_predictions)\n",
        "    targets = np.vstack(all_targets)\n",
        "    losses = np.concatenate(all_losses)\n",
        "    \n",
        "    # Calculate metrics\n",
        "    mse = mean_squared_error(targets.flatten(), predictions.flatten())\n",
        "    mae = mean_absolute_error(targets.flatten(), predictions.flatten())\n",
        "    r2 = r2_score(targets.flatten(), predictions.flatten())\n",
        "    \n",
        "    # Per-cell metrics\n",
        "    cell_mse = np.mean((predictions - targets) ** 2, axis=1)\n",
        "    cell_mae = np.mean(np.abs(predictions - targets), axis=1)\n",
        "    \n",
        "    # Correlation analysis\n",
        "    cell_correlations = []\n",
        "    for i in range(len(predictions)):\n",
        "        corr = np.corrcoef(predictions[i], targets[i])[0, 1]\n",
        "        if not np.isnan(corr):\n",
        "            cell_correlations.append(corr)\n",
        "    \n",
        "    results = {\n",
        "        'split_name': split_name,\n",
        "        'n_cells': len(predictions),\n",
        "        'n_genes': predictions.shape[1],\n",
        "        'mse': mse,\n",
        "        'mae': mae,\n",
        "        'r2': r2,\n",
        "        'mean_cell_mse': np.mean(cell_mse),\n",
        "        'std_cell_mse': np.std(cell_mse),\n",
        "        'mean_cell_mae': np.mean(cell_mae),\n",
        "        'std_cell_mae': np.std(cell_mae),\n",
        "        'mean_correlation': np.mean(cell_correlations) if cell_correlations else 0,\n",
        "        'std_correlation': np.std(cell_correlations) if cell_correlations else 0,\n",
        "        'predictions': predictions,\n",
        "        'targets': targets,\n",
        "        'losses': losses\n",
        "    }\n",
        "    \n",
        "    print(f\"  MSE: {mse:.6f}\")\n",
        "    print(f\"  MAE: {mae:.6f}\")\n",
        "    print(f\"  R²: {r2:.6f}\")\n",
        "    print(f\"  Mean cell correlation: {results['mean_correlation']:.4f}\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "def compare_model_performance(model_pretrain, model_finetune, train_adata, test_adata, val_adata):\n",
        "    \"\"\"Compare performance between pretrained and finetuned models\"\"\"\n",
        "    \n",
        "    print(\"=== MODEL PERFORMANCE COMPARISON ===\")\n",
        "    \n",
        "    results = {}\n",
        "    \n",
        "    # Evaluate pretrained model\n",
        "    print(\"\\n--- PRETRAINED MODEL EVALUATION ---\")\n",
        "    results['pretrain'] = {}\n",
        "    results['pretrain']['train'] = evaluate_model_performance(model_pretrain, train_adata, \"train\")\n",
        "    results['pretrain']['test'] = evaluate_model_performance(model_pretrain, test_adata, \"test\")\n",
        "    results['pretrain']['val'] = evaluate_model_performance(model_pretrain, val_adata, \"val\")\n",
        "    \n",
        "    # Evaluate finetuned model\n",
        "    print(\"\\n--- FINETUNED MODEL EVALUATION ---\")\n",
        "    results['finetune'] = {}\n",
        "    results['finetune']['train'] = evaluate_model_performance(model_finetune, train_adata, \"train\")\n",
        "    results['finetune']['test'] = evaluate_model_performance(model_finetune, test_adata, \"test\")\n",
        "    results['finetune']['val'] = evaluate_model_performance(model_finetune, val_adata, \"val\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "print(\"Evaluation functions defined!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run performance comparison\n",
        "print(\"Starting model performance evaluation...\")\n",
        "results = compare_model_performance(model_pretrain, model_finetune, train_adata, test_adata, val_adata)\n",
        "\n",
        "print(\"\\n=== EVALUATION COMPLETE ===\")\n",
        "print(\"Results summary:\")\n",
        "for model_type in ['pretrain', 'finetune']:\n",
        "    print(f\"\\n{model_type.upper()} MODEL:\")\n",
        "    for split in ['train', 'test', 'val']:\n",
        "        if results[model_type][split]:\n",
        "            r = results[model_type][split]\n",
        "            print(f\"  {split}: MSE={r['mse']:.6f}, R²={r['r2']:.6f}, Corr={r['mean_correlation']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comprehensive performance comparison plots\n",
        "def create_performance_comparison_plots(results):\n",
        "    \"\"\"Create comprehensive visualization of model performance comparison\"\"\"\n",
        "    \n",
        "    # Extract metrics for plotting\n",
        "    metrics = ['mse', 'mae', 'r2', 'mean_correlation']\n",
        "    metric_labels = ['MSE (Lower is Better)', 'MAE (Lower is Better)', 'R² (Higher is Better)', 'Mean Correlation (Higher is Better)']\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "    fig.suptitle('Model Performance Comparison: Pre-trained vs Fine-tuned', fontsize=16)\n",
        "    \n",
        "    # Data for plotting\n",
        "    model_types = ['Pretrained', 'Finetuned']\n",
        "    splits = ['Train', 'Test', 'Val']\n",
        "    \n",
        "    for idx, (metric, label) in enumerate(zip(metrics, metric_labels)):\n",
        "        row, col = idx // 2, idx % 2\n",
        "        ax = axes[row, col]\n",
        "        \n",
        "        # Extract data\n",
        "        pretrain_values = []\n",
        "        finetune_values = []\n",
        "        \n",
        "        for split in ['train', 'test', 'val']:\n",
        "            if results['pretrain'][split]:\n",
        "                pretrain_values.append(results['pretrain'][split][metric])\n",
        "            if results['finetune'][split]:\n",
        "                finetune_values.append(results['finetune'][split][metric])\n",
        "        \n",
        "        # Create grouped bar plot\n",
        "        x = np.arange(len(splits))\n",
        "        width = 0.35\n",
        "        \n",
        "        bars1 = ax.bar(x - width/2, pretrain_values, width, label='Pretrained', alpha=0.8)\n",
        "        bars2 = ax.bar(x + width/2, finetune_values, width, label='Finetuned', alpha=0.8)\n",
        "        \n",
        "        ax.set_xlabel('Dataset Split')\n",
        "        ax.set_ylabel(label)\n",
        "        ax.set_title(f'{label}')\n",
        "        ax.set_xticks(x)\n",
        "        ax.set_xticklabels(splits)\n",
        "        ax.legend()\n",
        "        \n",
        "        # Add value labels on bars\n",
        "        for bars in [bars1, bars2]:\n",
        "            for bar in bars:\n",
        "                height = bar.get_height()\n",
        "                ax.annotate(f'{height:.3f}',\n",
        "                           xy=(bar.get_x() + bar.get_width() / 2, height),\n",
        "                           xytext=(0, 3),  # 3 points vertical offset\n",
        "                           textcoords=\"offset points\",\n",
        "                           ha='center', va='bottom', fontsize=8)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Create performance improvement analysis\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "    fig.suptitle('Fine-tuning Performance Improvement Analysis', fontsize=16)\n",
        "    \n",
        "    for idx, split in enumerate(['train', 'test', 'val']):\n",
        "        ax = axes[idx]\n",
        "        \n",
        "        if results['pretrain'][split] and results['finetune'][split]:\n",
        "            pretrain_r = results['pretrain'][split]\n",
        "            finetune_r = results['finetune'][split]\n",
        "            \n",
        "            # Calculate improvements\n",
        "            mse_improvement = (pretrain_r['mse'] - finetune_r['mse']) / pretrain_r['mse'] * 100\n",
        "            r2_improvement = (finetune_r['r2'] - pretrain_r['r2']) / abs(pretrain_r['r2']) * 100\n",
        "            corr_improvement = (finetune_r['mean_correlation'] - pretrain_r['mean_correlation']) / abs(pretrain_r['mean_correlation']) * 100\n",
        "            \n",
        "            improvements = [mse_improvement, r2_improvement, corr_improvement]\n",
        "            labels = ['MSE Improvement (%)', 'R² Improvement (%)', 'Correlation Improvement (%)']\n",
        "            colors = ['red' if x < 0 else 'green' for x in improvements]\n",
        "            \n",
        "            bars = ax.bar(labels, improvements, color=colors, alpha=0.7)\n",
        "            ax.set_title(f'{split.title()} Split')\n",
        "            ax.set_ylabel('Improvement (%)')\n",
        "            ax.axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
        "            \n",
        "            # Add value labels\n",
        "            for bar, value in zip(bars, improvements):\n",
        "                height = bar.get_height()\n",
        "                ax.annotate(f'{value:.1f}%',\n",
        "                           xy=(bar.get_x() + bar.get_width() / 2, height),\n",
        "                           xytext=(0, 3 if height >= 0 else -15),\n",
        "                           textcoords=\"offset points\",\n",
        "                           ha='center', va='bottom' if height >= 0 else 'top', fontsize=10)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Create plots\n",
        "create_performance_comparison_plots(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detailed statistical analysis\n",
        "def perform_statistical_analysis(results):\n",
        "    \"\"\"Perform detailed statistical analysis of performance differences\"\"\"\n",
        "    \n",
        "    print(\"=== DETAILED STATISTICAL ANALYSIS ===\")\n",
        "    \n",
        "    analysis_results = {}\n",
        "    \n",
        "    for split in ['train', 'test', 'val']:\n",
        "        if results['pretrain'][split] and results['finetune'][split]:\n",
        "            print(f\"\\n--- {split.upper()} SPLIT ANALYSIS ---\")\n",
        "            \n",
        "            pretrain_r = results['pretrain'][split]\n",
        "            finetune_r = results['finetune'][split]\n",
        "            \n",
        "            # Extract cell-level metrics for statistical testing\n",
        "            pretrain_cell_mse = pretrain_r['losses']\n",
        "            finetune_cell_mse = finetune_r['losses']\n",
        "            \n",
        "            # Statistical tests\n",
        "            # 1. Paired t-test for MSE differences\n",
        "            from scipy.stats import ttest_rel, wilcoxon\n",
        "            \n",
        "            try:\n",
        "                t_stat, t_pvalue = ttest_rel(pretrain_cell_mse, finetune_cell_mse)\n",
        "                w_stat, w_pvalue = wilcoxon(pretrain_cell_mse, finetune_cell_mse)\n",
        "                \n",
        "                print(f\"Cell-level MSE comparison:\")\n",
        "                print(f\"  Pretrained mean MSE: {np.mean(pretrain_cell_mse):.6f} ± {np.std(pretrain_cell_mse):.6f}\")\n",
        "                print(f\"  Finetuned mean MSE:  {np.mean(finetune_cell_mse):.6f} ± {np.std(finetune_cell_mse):.6f}\")\n",
        "                print(f\"  Paired t-test: t={t_stat:.4f}, p={t_pvalue:.4f}\")\n",
        "                print(f\"  Wilcoxon test: W={w_stat:.4f}, p={w_pvalue:.4f}\")\n",
        "                \n",
        "                if t_pvalue < 0.05:\n",
        "                    improvement = (np.mean(pretrain_cell_mse) - np.mean(finetune_cell_mse)) / np.mean(pretrain_cell_mse) * 100\n",
        "                    print(f\"  *** SIGNIFICANT IMPROVEMENT: {improvement:.2f}% reduction in MSE ***\")\n",
        "                else:\n",
        "                    print(f\"  No significant difference in MSE\")\n",
        "                \n",
        "                analysis_results[split] = {\n",
        "                    't_stat': t_stat,\n",
        "                    't_pvalue': t_pvalue,\n",
        "                    'w_stat': w_stat,\n",
        "                    'w_pvalue': w_pvalue,\n",
        "                    'mse_improvement_pct': (np.mean(pretrain_cell_mse) - np.mean(finetune_cell_mse)) / np.mean(pretrain_cell_mse) * 100,\n",
        "                    'significant_improvement': t_pvalue < 0.05\n",
        "                }\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"Error in statistical analysis: {e}\")\n",
        "                analysis_results[split] = {}\n",
        "    \n",
        "    return analysis_results\n",
        "\n",
        "# Run statistical analysis\n",
        "statistical_results = perform_statistical_analysis(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate comprehensive summary report\n",
        "def generate_performance_summary(results, statistical_results):\n",
        "    \"\"\"Generate comprehensive summary of fine-tuning effectiveness\"\"\"\n",
        "    \n",
        "    print(\"=\" * 80)\n",
        "    print(\"COMPREHENSIVE FINE-TUNING PERFORMANCE SUMMARY\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # Overall performance comparison\n",
        "    print(\"\\n1. OVERALL PERFORMANCE COMPARISON:\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    for split in ['train', 'test', 'val']:\n",
        "        if results['pretrain'][split] and results['finetune'][split]:\n",
        "            pretrain_r = results['pretrain'][split]\n",
        "            finetune_r = results['finetune'][split]\n",
        "            \n",
        "            print(f\"\\n{split.upper()} SPLIT:\")\n",
        "            print(f\"  Pretrained - MSE: {pretrain_r['mse']:.6f}, R²: {pretrain_r['r2']:.4f}, Corr: {pretrain_r['mean_correlation']:.4f}\")\n",
        "            print(f\"  Finetuned  - MSE: {finetune_r['mse']:.6f}, R²: {finetune_r['r2']:.4f}, Corr: {finetune_r['mean_correlation']:.4f}\")\n",
        "            \n",
        "            # Calculate improvements\n",
        "            mse_improvement = (pretrain_r['mse'] - finetune_r['mse']) / pretrain_r['mse'] * 100\n",
        "            r2_improvement = (finetune_r['r2'] - pretrain_r['r2']) / abs(pretrain_r['r2']) * 100\n",
        "            corr_improvement = (finetune_r['mean_correlation'] - pretrain_r['mean_correlation']) / abs(pretrain_r['mean_correlation']) * 100\n",
        "            \n",
        "            print(f\"  Improvements - MSE: {mse_improvement:+.2f}%, R²: {r2_improvement:+.2f}%, Corr: {corr_improvement:+.2f}%\")\n",
        "    \n",
        "    # Statistical significance analysis\n",
        "    print(\"\\n2. STATISTICAL SIGNIFICANCE ANALYSIS:\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    significant_improvements = []\n",
        "    for split in ['train', 'test', 'val']:\n",
        "        if split in statistical_results and statistical_results[split]:\n",
        "            stats = statistical_results[split]\n",
        "            if stats.get('significant_improvement', False):\n",
        "                significant_improvements.append(split)\n",
        "                print(f\"  {split.upper()}: SIGNIFICANT improvement (p={stats['t_pvalue']:.4f}, {stats['mse_improvement_pct']:.2f}% MSE reduction)\")\n",
        "            else:\n",
        "                print(f\"  {split.upper()}: No significant improvement (p={stats['t_pvalue']:.4f})\")\n",
        "    \n",
        "    # Train vs Test performance analysis\n",
        "    print(\"\\n3. TRAIN vs TEST PERFORMANCE ANALYSIS:\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    if results['pretrain']['train'] and results['pretrain']['test'] and results['finetune']['train'] and results['finetune']['test']:\n",
        "        # Calculate generalization gap\n",
        "        pretrain_gap = results['pretrain']['test']['mse'] - results['pretrain']['train']['mse']\n",
        "        finetune_gap = results['finetune']['test']['mse'] - results['finetune']['train']['mse']\n",
        "        \n",
        "        print(f\"  Pretrained generalization gap (Test MSE - Train MSE): {pretrain_gap:.6f}\")\n",
        "        print(f\"  Finetuned generalization gap (Test MSE - Train MSE):  {finetune_gap:.6f}\")\n",
        "        \n",
        "        gap_improvement = (pretrain_gap - finetune_gap) / pretrain_gap * 100 if pretrain_gap != 0 else 0\n",
        "        print(f\"  Generalization gap improvement: {gap_improvement:+.2f}%\")\n",
        "        \n",
        "        if gap_improvement > 0:\n",
        "            print(f\"  *** FINE-TUNING REDUCED GENERALIZATION GAP ***\")\n",
        "        else:\n",
        "            print(f\"  Fine-tuning increased generalization gap\")\n",
        "    \n",
        "    # Overall assessment\n",
        "    print(\"\\n4. OVERALL ASSESSMENT:\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    # Count improvements\n",
        "    total_splits = len([s for s in ['train', 'test', 'val'] if results['pretrain'].get(s) and results['finetune'].get(s)])\n",
        "    improvement_count = 0\n",
        "    \n",
        "    for split in ['train', 'test', 'val']:\n",
        "        if results['pretrain'].get(split) and results['finetune'].get(split):\n",
        "            pretrain_r = results['pretrain'][split]\n",
        "            finetune_r = results['finetune'][split]\n",
        "            if finetune_r['mse'] < pretrain_r['mse']:\n",
        "                improvement_count += 1\n",
        "    \n",
        "    improvement_rate = improvement_count / total_splits * 100 if total_splits > 0 else 0\n",
        "    \n",
        "    print(f\"  Splits with MSE improvement: {improvement_count}/{total_splits} ({improvement_rate:.1f}%)\")\n",
        "    print(f\"  Splits with significant improvement: {len(significant_improvements)}/{total_splits}\")\n",
        "    \n",
        "    # Final verdict\n",
        "    print(f\"\\n5. FINAL VERDICT:\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    if len(significant_improvements) >= 2:\n",
        "        print(f\"  *** FINE-TUNING IS EFFECTIVE ***\")\n",
        "        print(f\"  - Significant improvements on {len(significant_improvements)} splits\")\n",
        "        print(f\"  - Training loss curve issues may be due to other factors\")\n",
        "    elif improvement_rate >= 66:\n",
        "        print(f\"  *** FINE-TUNING SHOWS MODERATE EFFECTIVENESS ***\")\n",
        "        print(f\"  - MSE improved on {improvement_rate:.1f}% of splits\")\n",
        "        print(f\"  - May need longer training or different hyperparameters\")\n",
        "    else:\n",
        "        print(f\"  *** FINE-TUNING EFFECTIVENESS IS LIMITED ***\")\n",
        "        print(f\"  - Only {improvement_rate:.1f}% of splits show improvement\")\n",
        "        print(f\"  - Training issues may indicate fundamental problems\")\n",
        "    \n",
        "    # Context from previous analysis\n",
        "    print(f\"\\n6. CONTEXT FROM DISTRIBUTION ANALYSIS:\")\n",
        "    print(\"-\" * 50)\n",
        "    print(f\"  - Previous analysis showed significant OOD issues between train/test\")\n",
        "    print(f\"  - 22 novel perturbations in test set not seen during training\")\n",
        "    print(f\"  - Gene expression distribution differences between splits\")\n",
        "    print(f\"  - These OOD issues may limit fine-tuning effectiveness\")\n",
        "    \n",
        "    return {\n",
        "        'improvement_rate': improvement_rate,\n",
        "        'significant_improvements': significant_improvements,\n",
        "        'generalization_gap_improvement': gap_improvement if 'gap_improvement' in locals() else 0,\n",
        "        'overall_verdict': 'effective' if len(significant_improvements) >= 2 else 'moderate' if improvement_rate >= 66 else 'limited'\n",
        "    }\n",
        "\n",
        "# Generate summary\n",
        "summary = generate_performance_summary(results, statistical_results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save comprehensive results\n",
        "print(\"\\n=== SAVING RESULTS ===\")\n",
        "\n",
        "# Create results directory\n",
        "results_dir = Path(\"./performance_comparison_results\")\n",
        "results_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# Save detailed results\n",
        "import json\n",
        "\n",
        "# Convert numpy arrays to lists for JSON serialization\n",
        "def convert_for_json(obj):\n",
        "    if isinstance(obj, np.ndarray):\n",
        "        return obj.tolist()\n",
        "    elif isinstance(obj, np.integer):\n",
        "        return int(obj)\n",
        "    elif isinstance(obj, np.floating):\n",
        "        return float(obj)\n",
        "    elif isinstance(obj, dict):\n",
        "        return {k: convert_for_json(v) for k, v in obj.items()}\n",
        "    elif isinstance(obj, list):\n",
        "        return [convert_for_json(item) for item in obj]\n",
        "    else:\n",
        "        return obj\n",
        "\n",
        "# Save performance results (without large arrays)\n",
        "performance_summary = {}\n",
        "for model_type in ['pretrain', 'finetune']:\n",
        "    performance_summary[model_type] = {}\n",
        "    for split in ['train', 'test', 'val']:\n",
        "        if results[model_type][split]:\n",
        "            r = results[model_type][split]\n",
        "            performance_summary[model_type][split] = {\n",
        "                'split_name': r['split_name'],\n",
        "                'n_cells': r['n_cells'],\n",
        "                'n_genes': r['n_genes'],\n",
        "                'mse': r['mse'],\n",
        "                'mae': r['mae'],\n",
        "                'r2': r['r2'],\n",
        "                'mean_cell_mse': r['mean_cell_mse'],\n",
        "                'std_cell_mse': r['std_cell_mse'],\n",
        "                'mean_cell_mae': r['mean_cell_mae'],\n",
        "                'std_cell_mae': r['std_cell_mae'],\n",
        "                'mean_correlation': r['mean_correlation'],\n",
        "                'std_correlation': r['std_correlation']\n",
        "            }\n",
        "\n",
        "with open(results_dir / \"performance_comparison.json\", \"w\") as f:\n",
        "    json.dump(performance_summary, f, indent=2)\n",
        "\n",
        "# Save statistical results\n",
        "with open(results_dir / \"statistical_analysis.json\", \"w\") as f:\n",
        "    json.dump(convert_for_json(statistical_results), f, indent=2)\n",
        "\n",
        "# Save summary\n",
        "with open(results_dir / \"summary_report.json\", \"w\") as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "# Save plots\n",
        "plt.savefig(results_dir / \"performance_comparison.png\", dpi=300, bbox_inches='tight')\n",
        "\n",
        "print(f\"Results saved to {results_dir}/\")\n",
        "print(f\"Files created:\")\n",
        "print(f\"  - performance_comparison.json\")\n",
        "print(f\"  - statistical_analysis.json\") \n",
        "print(f\"  - summary_report.json\")\n",
        "print(f\"  - performance_comparison.png\")\n",
        "\n",
        "print(f\"\\n=== ANALYSIS COMPLETE ===\")\n",
        "print(f\"Fine-tuning effectiveness verdict: {summary['overall_verdict'].upper()}\")\n",
        "print(f\"Improvement rate: {summary['improvement_rate']:.1f}%\")\n",
        "print(f\"Significant improvements: {len(summary['significant_improvements'])} splits\")\n",
        "\n",
        "print(f\"\\nThis comprehensive analysis provides quantitative evidence of fine-tuning effectiveness\")\n",
        "print(f\"and helps understand whether training loss curve issues indicate actual performance problems.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
