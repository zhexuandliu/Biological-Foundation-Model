{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","mount_file_id":"1Lob4pwOQbpi97J9o5nqWejQHSZ_V-e6v","authorship_tag":"ABX9TyPeyNzkCEPqH/3Gz62GTpFo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":16,"metadata":{"id":"IgJQbRp4-uB2","executionInfo":{"status":"ok","timestamp":1763512320777,"user_tz":360,"elapsed":12,"user":{"displayName":"Zhexuan Liu","userId":"18073944608501350772"}}},"outputs":[],"source":["import numpy as np\n","import torch\n","from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoModel, AutoModelForCausalLM, BertConfig\n","import sys\n","\n","# class NoModule:\n","#     def __init__(self, *module_names):\n","#         self.module_names = module_names\n","#         self.original_modules = {}\n","\n","#     def __enter__(self):\n","#         for module_name in self.module_names:\n","#             if module_name in sys.modules:\n","#                 # Save the original module\n","#                 self.original_modules[module_name] = sys.modules[module_name]\n","#                 # Remove it so imports behave as if it's not installed\n","#                 del sys.modules[module_name]\n","\n","#     def __exit__(self, exc_type, exc_value, traceback):\n","#         # Restore any original modules\n","#         for module_name in self.module_names:\n","#             if module_name in self.original_modules:\n","#                 sys.modules[module_name] = self.original_modules[module_name]\n","#             else:\n","#                 # If we created no original entry and something\n","#                 # imported it in the meantime, leave it alone\n","#                 pass\n","\n","import builtins\n","\n","class BlockImport:\n","    def __init__(self, *blocked):\n","        self.blocked = set(blocked)\n","\n","    def __enter__(self):\n","        self._orig_import = builtins.__import__\n","\n","        def fake_import(name, *args, **kwargs):\n","            if any(name == b or name.startswith(b + \".\") for b in self.blocked):\n","                raise ImportError(f\"Blocked import of {name}\")\n","            return self._orig_import(name, *args, **kwargs)\n","\n","        builtins.__import__ = fake_import\n","\n","    def __exit__(self, exc_type, exc_value, traceback):\n","        builtins.__import__ = self._orig_import\n","\n","\n","class dnalm_embedding_extraction():\n","    def __init__(self, model_class, model_name, device):\n","        self.model_class = model_class\n","        if model_class==\"DNABERT2\":\n","            self.model_name = f\"zhihan1996/{model_name}\"\n","            with BlockImport(\"triton\"):\n","                self.tokenizer = AutoTokenizer.from_pretrained(\n","                    self.model_name, trust_remote_code=True\n","                )\n","                self.model = AutoModel.from_pretrained(\n","                    self.model_name, trust_remote_code=True\n","                )\n","        elif model_class==\"HyenaDNA\":\n","            self.model_name = f\"LongSafari/{model_name}\"\n","            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name, trust_remote_code=True, padding_side=\"right\")\n","            self.model = AutoModelForCausalLM.from_pretrained(self.model_name, trust_remote_code=True)\n","        elif model_class==\"Nucleotide Transformer\":\n","            self.model_name = f\"InstaDeepAI/{model_name}\"\n","            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name, trust_remote_code=True)\n","            self.model = AutoModelForMaskedLM.from_pretrained(self.model_name, trust_remote_code=True)\n","        else:\n","          print(\"Model not supported.\")\n","        self.device = device\n","        self.model.to(self.device)\n","        self.model.eval()\n","\n","    def get_embedding(self, sequences, batch_size):\n","        embeddings = []\n","        for i in range(0, len(sequences), batch_size):\n","            batch = sequences[i:min(i+batch_size, len(sequences))]\n","\n","            if self.model_class==\"Nucleotide Transformer\":\n","                enc = self.tokenizer.batch_encode_plus(\n","                    batch,\n","                    return_tensors=\"pt\",\n","                    padding=True,\n","                    max_length=self.tokenizer.model_max_length,\n","                )\n","                input_ids = enc[\"input_ids\"].to(self.device)\n","                attention_mask = enc[\"attention_mask\"].to(self.device)\n","                with torch.no_grad():\n","                    outputs = self.model(\n","                        input_ids,\n","                        attention_mask=attention_mask,\n","                        output_hidden_states=True\n","                    )\n","                    hidden = outputs.hidden_states[-1]\n","                    mask_expanded = attention_mask.unsqueeze(-1)\n","                    summed = (hidden * mask_expanded).sum(dim=1)\n","                    counts = mask_expanded.sum(dim=1)\n","                    mean_embeddings = summed / counts\n","            elif self.model_class==\"HyenaDNA\":\n","                enc = self.tokenizer.batch_encode_plus(\n","                    batch,\n","                    return_tensors=\"pt\",\n","                    padding=\"longest\",\n","                    truncation=True,\n","                    max_length=self.tokenizer.model_max_length,\n","                )\n","                input_ids = enc[\"input_ids\"].to(self.device)\n","                pad_id = self.tokenizer.pad_token_id\n","                if pad_id is None:\n","                    attention_mask = torch.ones_like(input_ids, device=self.device)\n","                else:\n","                    attention_mask = (input_ids != pad_id).long().to(self.device)\n","                with torch.no_grad():\n","                    outputs = self.model(\n","                        input_ids=input_ids,\n","                        output_hidden_states=True,\n","                    )\n","                    hidden = outputs.hidden_states[-1]\n","                    mask_expanded = attention_mask.unsqueeze(-1)\n","\n","                    summed = (hidden * mask_expanded).sum(dim=1)\n","                    counts = mask_expanded.sum(dim=1).clamp(min=1e-9)\n","                    mean_embeddings = summed / counts\n","\n","            elif self.model_class==\"DNABERT2\":\n","                enc = self.tokenizer(\n","                    batch,\n","                    return_tensors=\"pt\",\n","                    padding=True,\n","                    truncation=True\n","                )\n","                input_ids = enc[\"input_ids\"].to(self.device)\n","                attention_mask = enc[\"attention_mask\"].to(self.device)\n","\n","                with torch.no_grad():\n","                    outputs = self.model(\n","                        input_ids=input_ids,\n","                        attention_mask=attention_mask,\n","                    )\n","                    hidden = outputs[0]\n","                    mask_expanded = attention_mask.unsqueeze(-1).float()\n","                    summed = (hidden * mask_expanded).sum(dim=1)\n","                    counts = mask_expanded.sum(dim=1).clamp(min=1e-9)\n","                    mean_embeddings = summed / counts\n","\n","            embeddings.append(mean_embeddings.cpu().numpy())\n","        return np.vstack(embeddings)\n","\n","\n"]},{"cell_type":"code","source":["# def MLP model\n","import torch\n","import torch.nn as nn\n","\n","# Define the MLP model for Binary Classification\n","class MLPBinary(nn.Module):\n","    def __init__(self, input_size):\n","        super(MLPBinary, self).__init__()\n","        self.fc1 = nn.Linear(input_size, 256)  # First hidden layer\n","        self.relu1 = nn.ReLU()\n","        self.fc2 = nn.Linear(256, 128) # Second hidden layer\n","        self.relu2 = nn.ReLU()\n","        self.fc3 = nn.Linear(128, 1)   # Output layer for binary classification\n","\n","    def forward(self, x):\n","        x = self.fc1(x)\n","        x = self.relu1(x)\n","        x = self.fc2(x)\n","        x = self.relu2(x)\n","        x = self.fc3(x) # No sigmoid here, as BCEWithLogitsLoss will be used\n","        return x"],"metadata":{"id":"X_qDNdG8IYmw","executionInfo":{"status":"ok","timestamp":1763512156850,"user_tz":360,"elapsed":3,"user":{"displayName":"Zhexuan Liu","userId":"18073944608501350772"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import precision_recall_curve, average_precision_score\n","from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","\n","def performance_cCREs(embeddings, label):\n","    X = embeddings\n","    y = label\n","\n","    # train/val split (stratified)\n","    X_train, X_test, y_train, y_test = train_test_split(\n","        X, y, test_size=0.3, random_state=40, stratify=y\n","    )\n","\n","    # # scale inputs\n","    # scaler = StandardScaler()\n","    # X_train = scaler.fit_transform(X_train)\n","    # X_test  = scaler.transform(X_test)\n","\n","    # tensors\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    X_train_t = torch.from_numpy(X_train).float().to(device)\n","    y_train_t = torch.from_numpy(y_train).float().reshape(-1, 1).to(device)\n","    X_test_t  = torch.from_numpy(X_test).float().to(device)\n","\n","    # model: small, regularized\n","    input_size = X_train.shape[1]\n","    model = MLPBinary(input_size).to(device)  # ensure this has Dropout/BatchNorm or is small\n","\n","    # class imbalance\n","    neg, pos = np.bincount(y_train)\n","    pos_weight = torch.tensor(neg / max(pos, 1), dtype=torch.float, device=device)\n","\n","    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n","    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n","\n","    # early stopping on val AUPRC\n","    best_ap, patience, wait = -np.inf, 20, 0\n","    num_epochs = 2000\n","    model.train()\n","    for epoch in range(num_epochs):\n","        optimizer.zero_grad()\n","        logits = model(X_train_t)\n","        loss = criterion(logits, y_train_t)\n","        loss.backward()\n","        optimizer.step()\n","\n","    model.eval()\n","    with torch.no_grad():\n","        probs = torch.sigmoid(model(X_test_t)).cpu().numpy().ravel()\n","\n","    precision, recall, _ = precision_recall_curve(y_test, probs)\n","    ap = average_precision_score(y_test, probs)\n","\n","    y_pred = (probs >= 0.5).astype(int)\n","    prec_cls = precision_score(y_test, y_pred, zero_division=0)\n","    rec_cls  = recall_score(y_test, y_pred, zero_division=0)\n","    f1_cls   = f1_score(y_test, y_pred, zero_division=0)\n","    acc_cls  = accuracy_score(y_test, y_pred)\n","\n","    return (prec_cls, rec_cls, f1_cls, acc_cls)\n"],"metadata":{"id":"78yl3o2teltQ","executionInfo":{"status":"ok","timestamp":1763512158191,"user_tz":360,"elapsed":2,"user":{"displayName":"Zhexuan Liu","userId":"18073944608501350772"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","\n","# Path to your file\n","file_path = \"/content/drive/MyDrive/GitHub/Biological-Foundation-Model/Notebooks/EP Pair Evaluation/accessary_files/cCRE_pos_neg_dataset.csv\"\n","\n","# Read the compressed TSV file\n","df = pd.read_csv(file_path)\n","\n","df_sub = (\n","    df.groupby(\"label\", group_keys=False)\n","      .apply(lambda x: x.sample(n = 5000, random_state=42))\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cipCEbIC_KHS","executionInfo":{"status":"ok","timestamp":1763512179371,"user_tz":360,"elapsed":19313,"user":{"displayName":"Zhexuan Liu","userId":"18073944608501350772"}},"outputId":"6b309d8f-2371-47d0-8d2d-d07cc860cad0"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-2345874753.py:11: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n","  .apply(lambda x: x.sample(n = 5000, random_state=42))\n"]}]},{"cell_type":"code","source":["embedding_extractor_hyenaDNA = dnalm_embedding_extraction(model_class=\"HyenaDNA\", model_name=\"hyenadna-large-1m-seqlen-hf\", device=torch.device(\"cuda\"))\n","embedding_extractor_dnabert2 = dnalm_embedding_extraction(model_class=\"DNABERT2\", model_name=\"DNABERT-2-117M\", device=torch.device(\"cuda\"))\n","embedding_extractor_nt = dnalm_embedding_extraction(model_class=\"Nucleotide Transformer\", model_name=\"nucleotide-transformer-v2-500m-multi-species\", device=torch.device(\"cuda\"))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fbgyy5yZ_MGK","executionInfo":{"status":"ok","timestamp":1763512327817,"user_tz":360,"elapsed":3431,"user":{"displayName":"Zhexuan Liu","userId":"18073944608501350772"}},"outputId":"db94bf39-c4ec-4c7c-f0c2-02f93360aa99"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stderr","text":["/root/.cache/huggingface/modules/transformers_modules/zhihan1996/DNABERT_hyphen_2_hyphen_117M/7bce263b15377fc15361f52cfab88f8b586abda0/bert_layers.py:126: UserWarning: Unable to import Triton; defaulting MosaicBERT attention implementation to pytorch (this will reduce throughput when using this model).\n","  warnings.warn(\n","Some weights of BertModel were not initialized from the model checkpoint at zhihan1996/DNABERT-2-117M and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","source":["for embedding_extractor in [embedding_extractor_hyenaDNA, embedding_extractor_dnabert2, embedding_extractor_nt]:\n","    df_sub['embedding_mean'] = list(embedding_extractor.get_embedding(sequences=df_sub[\"sequence\"].tolist(), batch_size=25))\n","    prec_cls, rec_cls, f1_cls, acc_cls = performance_cCREs(np.vstack(df_sub['embedding_mean']).astype(np.float32), df_sub[\"label\"].astype(int).values)\n","    print(embedding_extractor.model_class)\n","    print(f\"  Precision: {prec_cls:.3f}\")\n","    print(f\"  Recall   : {rec_cls:.3f}\")\n","    print(f\"  F1 score : {f1_cls:.3f}\")\n","    print(f\"  Accuracy : {acc_cls:.3f}\\n\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fJ9Bq68jeik1","executionInfo":{"status":"ok","timestamp":1763512403374,"user_tz":360,"elapsed":75542,"user":{"displayName":"Zhexuan Liu","userId":"18073944608501350772"}},"outputId":"f2b9c29b-b9e4-4547-b9c4-6713e76424a2"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stderr","text":["Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"]},{"output_type":"stream","name":"stdout","text":["HyenaDNA\n","  Precision: 0.668\n","  Recall   : 0.783\n","  F1 score : 0.721\n","  Accuracy : 0.697\n","\n","DNABERT2\n","  Precision: 0.797\n","  Recall   : 0.789\n","  F1 score : 0.793\n","  Accuracy : 0.794\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/transformers/tokenization_utils_base.py:2779: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Nucleotide Transformer\n","  Precision: 0.737\n","  Recall   : 0.727\n","  F1 score : 0.732\n","  Accuracy : 0.734\n","\n"]}]}]}