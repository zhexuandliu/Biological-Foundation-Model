{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","mount_file_id":"1Lob4pwOQbpi97J9o5nqWejQHSZ_V-e6v","authorship_tag":"ABX9TyMlsNvMFmJHL3exn2SDPR4J"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# !pip install mamba_ssm"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZH2fhQdeJCdM","executionInfo":{"status":"ok","timestamp":1763740983535,"user_tz":360,"elapsed":56573,"user":{"displayName":"Zhexuan Liu","userId":"18073944608501350772"}},"outputId":"6e163386-466d-4a35-e4c0-27c55774ba39"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting mamba_ssm\n","  Using cached mamba_ssm-2.2.6.post3.tar.gz (113 kB)\n","\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n","\u001b[0m  Installing build dependencies ... \u001b[?25l\u001b[?25hcanceled"]}]},{"cell_type":"code","source":["import numpy as np\n","import torch\n","from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoModel, AutoModelForCausalLM, BertConfig\n","import sys\n","import transformers.models.bert.modeling_bert\n","import builtins\n","\n","class BlockImport:\n","    def __init__(self, *blocked):\n","        self.blocked = set(blocked)\n","\n","    def __enter__(self):\n","        self._orig_import = builtins.__import__\n","\n","        def fake_import(name, *args, **kwargs):\n","            if any(name == b or name.startswith(b + \".\") for b in self.blocked):\n","                raise ImportError(f\"Blocked import of {name}\")\n","            return self._orig_import(name, *args, **kwargs)\n","\n","        builtins.__import__ = fake_import\n","\n","    def __exit__(self, exc_type, exc_value, traceback):\n","        builtins.__import__ = self._orig_import\n","\n","\n","class dnalm_embedding_extraction():\n","    def __init__(self, model_class, model_name, device):\n","        self.model_class = model_class\n","        if model_class==\"DNABERT2\":\n","            self.model_name = f\"zhihan1996/{model_name}\"\n","            # with NoModule(\"triton\"):\n","            # with NoTriton():\n","            with BlockImport(\"triton\"):\n","                self.tokenizer = AutoTokenizer.from_pretrained(\n","                    self.model_name, trust_remote_code=True\n","                )\n","                config = BertConfig.from_pretrained(self.model_name, trust_remote_code=True)\n","                self.model = AutoModelForMaskedLM.from_pretrained(self.model_name, config=config, trust_remote_code=True)\n","                self.mask_token = self.tokenizer.mask_token_id\n","        elif model_class==\"HyenaDNA\":\n","            self.model_name = f\"LongSafari/{model_name}\"\n","            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name, trust_remote_code=True, padding_side=\"right\")\n","            self.model = AutoModelForCausalLM.from_pretrained(self.model_name, trust_remote_code=True)\n","\n","        elif model_class==\"Nucleotide Transformer\":\n","            self.model_name = f\"InstaDeepAI/{model_name}\"\n","            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name, trust_remote_code=True)\n","            self.model = AutoModelForMaskedLM.from_pretrained(self.model_name, trust_remote_code=True)\n","            self.mask_token = self.tokenizer.mask_token_id\n","        elif model_class==\"Caduceus\":\n","            self.model_name = f\"kuleshov-group/{model_name}\"\n","            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name, trust_remote_code=True, padding_side=\"right\")\n","            self.model = AutoModelForMaskedLM.from_pretrained(self.model_name, trust_remote_code=True)\n","            self.mask_token = self.tokenizer.mask_token_id\n","        elif model_class==\"Mistral\":\n","            self.model_name = f\"RaphaelMourad/{model_name}\"\n","            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name, trust_remote_code=True)\n","            self.model = AutoModelForCausalLM.from_pretrained(self.model_name, trust_remote_code=True)\n","            self.mask_token = self.tokenizer.mask_token_id\n","        elif model_class==\"GENA-LM\":\n","            self.model_name = f\"AIRI-Institute/{model_name}\"\n","            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name, trust_remote_code=True)\n","            self.model = AutoModel.from_pretrained(self.model_name, trust_remote_code=True)\n","            self.mask_token = self.tokenizer.mask_token_id\n","        else:\n","          print(\"Model not supported.\")\n","        self.device = device\n","        self.model.to(self.device)\n","        self.model.eval()\n","\n","\n","    @property\n","    def start_token(self):\n","        if self.model_class==\"HyenaDNA\":\n","            return None\n","        elif self.model_class==\"DNABERT2\":\n","            return 1\n","        elif self.model_class==\"Nucleotide Transformer\":\n","            return 3\n","        elif self.model_class==\"Caduceus\":\n","            return None\n","        elif self.model_class==\"Mistral\":\n","            return 1\n","        elif self.model_class==\"GENA-LM\":\n","            return 1\n","\n","    @property\n","    def end_token(self):\n","        if self.model_class==\"HyenaDNA\":\n","            return 1\n","        elif self.model_class==\"DNABERT2\":\n","            return 2\n","        elif self.model_class==\"Nucleotide Transformer\":\n","            return None\n","        elif self.model_class==\"Caduceus\":\n","            return 1\n","        elif self.model_class==\"Mistral\":\n","            return 2\n","        elif self.model_class==\"GENA-LM\":\n","            return 2\n","\n","    def get_embedding(self, sequences, batch_size):\n","        embeddings = []\n","        for i in range(0, len(sequences), batch_size):\n","            # if i%50000==0:\n","            #     print(i)\n","            batch = sequences[i:min(i+batch_size, len(sequences))]\n","\n","            if self.model_class==\"Nucleotide Transformer\":\n","                encoded = self.tokenizer.batch_encode_plus(batch, return_tensors=\"pt\", padding=True)\n","                tokens = encoded[\"input_ids\"]\n","                attention_mask = encoded.get(\"attention_mask\")\n","                if self.start_token is not None:\n","                    starts = torch.where(tokens == self.start_token)[1] + 1\n","                else:\n","                    starts = 0\n","                if self.end_token is not None:\n","                    ends = torch.where(tokens == self.end_token)[1]\n","                else:\n","                    ends = attention_mask.sum(dim=1)\n","\n","                tokens = tokens.to(device=self.device)\n","                if attention_mask is not None:\n","                    attention_mask = attention_mask.to(device=self.device)\n","\n","                with torch.no_grad():\n","                    torch_outs = self.model(\n","                        tokens,\n","                        attention_mask=attention_mask,\n","                        output_hidden_states=True,\n","                        return_dict=True,\n","                    )\n","\n","                clip_mask = torch.zeros(tokens.shape[:2], device=self.device)\n","                for i in range(clip_mask.shape[1]):\n","                    clip_mask[:,i] = ((i >= starts) & (i < ends))\n","                if attention_mask is not None:\n","                    clip_mask = clip_mask * attention_mask\n","\n","                hidden = torch_outs.hidden_states[-1]\n","                mask = clip_mask.unsqueeze(-1)\n","                summed = (hidden * mask).sum(dim=1)\n","                counts = mask.sum(dim=1).clamp(min=1e-9)\n","                mean_embeddings = summed / counts\n","\n","            elif self.model_class==\"Mistral\":\n","                encoded = self.tokenizer.batch_encode_plus(batch, return_tensors=\"pt\", padding=True)\n","                tokens = encoded[\"input_ids\"]\n","                attention_mask = encoded.get(\"attention_mask\")\n","                if self.start_token is not None:\n","                    starts = torch.where(tokens == self.start_token)[1] + 1\n","                else:\n","                    starts = 0\n","                if self.end_token is not None:\n","                    ends = torch.where(tokens == self.end_token)[1]\n","                else:\n","                    ends = attention_mask.sum(dim=1)\n","\n","                tokens = tokens.to(device=self.device)\n","                if attention_mask is not None:\n","                    attention_mask = attention_mask.to(device=self.device)\n","\n","                with torch.no_grad():\n","                    torch_outs = self.model(\n","                        tokens,\n","                        output_hidden_states=True,\n","                        return_dict=True,\n","                    )\n","\n","                clip_mask = torch.zeros(tokens.shape[:2], device=self.device)\n","                for i in range(clip_mask.shape[1]):\n","                    clip_mask[:,i] = ((i >= starts) & (i < ends))\n","                if attention_mask is not None:\n","                    clip_mask = clip_mask * attention_mask\n","\n","                hidden = torch_outs.hidden_states[-1]\n","\n","                mask = clip_mask.unsqueeze(-1)\n","                summed = (hidden * mask).sum(dim=1)\n","                counts = mask.sum(dim=1).clamp(min=1e-9)\n","                mean_embeddings = summed / counts\n","\n","            elif self.model_class==\"HyenaDNA\":\n","                encoded = self.tokenizer.batch_encode_plus(batch, return_tensors=\"pt\", padding=True)\n","                tokens = encoded[\"input_ids\"]\n","                attention_mask = encoded.get(\"attention_mask\")\n","                if self.start_token is not None:\n","                    starts = torch.where(tokens == self.start_token)[1] + 1\n","                else:\n","                    starts = 0\n","                if self.end_token is not None:\n","                    ends = torch.where(tokens == self.end_token)[1]\n","                else:\n","                    ends = attention_mask.sum(dim=1)\n","\n","                tokens = tokens.to(device=self.device)\n","                if attention_mask is not None:\n","                    attention_mask = attention_mask.to(device=self.device)\n","\n","                with torch.no_grad():\n","                    torch_outs = self.model(\n","                        tokens,\n","                        output_hidden_states=True,\n","                        return_dict=True,\n","                    )\n","\n","                clip_mask = torch.zeros(tokens.shape[:2], device=self.device)\n","                for i in range(clip_mask.shape[1]):\n","                    clip_mask[:,i] = ((i >= starts) & (i < ends))\n","                if attention_mask is not None:\n","                    clip_mask = clip_mask * attention_mask\n","\n","                hidden = torch_outs.hidden_states[-1]\n","\n","                mask = clip_mask.unsqueeze(-1)\n","                summed = (hidden * mask).sum(dim=1)\n","                counts = mask.sum(dim=1).clamp(min=1e-9)\n","                mean_embeddings = summed / counts\n","\n","            elif self.model_class==\"DNABERT2\":\n","                encoded = self.tokenizer.batch_encode_plus(batch, return_tensors=\"pt\", padding=True)\n","                tokens = encoded[\"input_ids\"]\n","                attention_mask = encoded.get(\"attention_mask\")\n","                if self.start_token is not None:\n","                    starts = torch.where(tokens == self.start_token)[1] + 1\n","                else:\n","                    starts = 0\n","                if self.end_token is not None:\n","                    ends = torch.where(tokens == self.end_token)[1]\n","                else:\n","                    ends = attention_mask.sum(dim=1)\n","\n","                tokens = tokens.to(device=self.device)\n","                if attention_mask is not None:\n","                    attention_mask = attention_mask.to(device=self.device)\n","\n","                with torch.no_grad():\n","                    torch_outs = self.model(\n","                        tokens,\n","                        attention_mask=attention_mask,\n","                        output_hidden_states=True,\n","                        return_dict=True,\n","                    )\n","\n","                clip_mask = torch.zeros(tokens.shape[:2], device=self.device)\n","                for i in range(clip_mask.shape[1]):\n","                    clip_mask[:,i] = ((i >= starts) & (i < ends))\n","                if attention_mask is not None:\n","                    clip_mask = clip_mask * attention_mask\n","\n","                # !!! due to the bug in its code, DNABERT2 can only return the last hidden layer\n","                hidden = torch_outs.hidden_states\n","                mask = clip_mask.unsqueeze(-1)\n","                summed = (hidden * mask).sum(dim=1)\n","                counts = mask.sum(dim=1).clamp(min=1e-9)\n","                mean_embeddings = summed / counts\n","\n","            elif self.model_class==\"Caduceus\":\n","                encoded = self.tokenizer.batch_encode_plus(batch, return_tensors=\"pt\", padding=True)\n","                tokens = encoded[\"input_ids\"]\n","                attention_mask = encoded.get(\"attention_mask\")\n","                if self.start_token is not None:\n","                    starts = torch.where(tokens == self.start_token)[1] + 1\n","                else:\n","                    starts = 0\n","                if self.end_token is not None:\n","                    ends = torch.where(tokens == self.end_token)[1]\n","                else:\n","                    ends = attention_mask.sum(dim=1)\n","\n","                tokens = tokens.to(device=self.device)\n","                if attention_mask is not None:\n","                    attention_mask = attention_mask.to(device=self.device)\n","\n","                with torch.no_grad():\n","                    torch_outs = self.model(\n","                        tokens,\n","                        attention_mask=attention_mask,\n","                        output_hidden_states=True,\n","                        return_dict=True,\n","                    )\n","\n","                clip_mask = torch.zeros(tokens.shape[:2], device=self.device)\n","                for i in range(clip_mask.shape[1]):\n","                    clip_mask[:,i] = ((i >= starts) & (i < ends))\n","                if attention_mask is not None:\n","                    clip_mask = clip_mask * attention_mask\n","\n","                hidden = torch_outs.hidden_states[-1]\n","                mask = clip_mask.unsqueeze(-1)\n","                summed = (hidden * mask).sum(dim=1)\n","                counts = mask.sum(dim=1).clamp(min=1e-9)\n","                mean_embeddings = summed / counts\n","\n","            elif self.model_class==\"GENA-LM\":\n","                encoded = self.tokenizer.batch_encode_plus(batch, return_tensors=\"pt\", padding=True)\n","                tokens = encoded[\"input_ids\"]\n","                attention_mask = encoded.get(\"attention_mask\")\n","                if self.start_token is not None:\n","                    starts = torch.where(tokens == self.start_token)[1] + 1\n","                else:\n","                    starts = 0\n","                if self.end_token is not None:\n","                    ends = torch.where(tokens == self.end_token)[1]\n","                else:\n","                    ends = attention_mask.sum(dim=1)\n","\n","                tokens = tokens.to(device=self.device)\n","                if attention_mask is not None:\n","                    attention_mask = attention_mask.to(device=self.device)\n","\n","                with torch.no_grad():\n","                    torch_outs = self.model(\n","                        tokens,\n","                        attention_mask=attention_mask,\n","                        output_hidden_states=True,\n","                        return_dict=True,\n","                    )\n","\n","                clip_mask = torch.zeros(tokens.shape[:2], device=self.device)\n","                for i in range(clip_mask.shape[1]):\n","                    clip_mask[:,i] = ((i >= starts) & (i < ends))\n","                if attention_mask is not None:\n","                    clip_mask = clip_mask * attention_mask\n","\n","                hidden = torch_outs.hidden_states[-1]\n","                mask = clip_mask.unsqueeze(-1)\n","                summed = (hidden * mask).sum(dim=1)\n","                counts = mask.sum(dim=1).clamp(min=1e-9)\n","                mean_embeddings = summed / counts\n","\n","            embeddings.append(mean_embeddings.cpu().numpy())\n","        return np.vstack(embeddings)\n","\n","\n","    def get_likelihood(self, sequences, batch_size):\n","        \"\"\"\n","        Compute log-likelihoods of sequences.\n","        Returns: numpy array of log-likelihoods (one per sequence)\n","        \"\"\"\n","        import torch.nn.functional as F\n","\n","        likelihoods = []\n","\n","        for i in range(0, len(sequences), batch_size):\n","            # if i % 50000 == 0:\n","            #     print(i)\n","            batch = sequences[i:min(i+batch_size, len(sequences))]\n","\n","            if self.model_class == \"Nucleotide Transformer\":\n","\n","                encoded = self.tokenizer.batch_encode_plus(batch, return_tensors=\"pt\", padding=True)\n","                tokens = encoded[\"input_ids\"]\n","                attention_mask = encoded.get(\"attention_mask\")\n","                if self.start_token is not None:\n","                    starts = torch.where(tokens == self.start_token)[1] + 1\n","                else:\n","                    starts = 0\n","                if self.end_token is not None:\n","                    ends = torch.where(tokens == self.end_token)[1]\n","                else:\n","                    ends = attention_mask.sum(dim=1)\n","\n","                tokens = tokens.to(device=self.device)\n","                if attention_mask is not None:\n","                    attention_mask = attention_mask.to(device=self.device)\n","                lls = torch.zeros(tokens.shape[:2], device=self.device)\n","                for i in range(tokens.shape[1]):\n","                    clip_mask = ((i >= starts) & (i < ends)).to(device=self.device)\n","                    masked_tokens = tokens.clone()\n","                    masked_tokens[:,i,...] = self.mask_token\n","                    with torch.no_grad():\n","                        torch_outs = self.model(\n","                            masked_tokens,\n","                            attention_mask=attention_mask,\n","                        )\n","                        logits = torch_outs.logits.swapaxes(1, 2)\n","                        tmp = -F.cross_entropy(logits, tokens, reduction=\"none\")\n","                        lls[:,i] = tmp[:,i] * clip_mask\n","\n","                seq_likelihoods = lls.sum(dim=1).numpy(force=True)\n","\n","            elif self.model_class == \"Mistral\":\n","                encoded = self.tokenizer.batch_encode_plus(batch, return_tensors=\"pt\", padding=True)\n","                tokens = encoded[\"input_ids\"]\n","                attention_mask = encoded.get(\"attention_mask\")\n","                if self.start_token is not None:\n","                    starts = torch.where(tokens == self.start_token)[1] + 1\n","                else:\n","                    starts = 0\n","                if self.end_token is not None:\n","                    ends = torch.where(tokens == self.end_token)[1]\n","                else:\n","                    ends = attention_mask.sum(dim=1)\n","\n","                tokens = tokens.to(device=self.device)\n","                if attention_mask is not None:\n","                    attention_mask = attention_mask.to(device=self.device)\n","\n","                with torch.no_grad():\n","                    torch_outs = self.model(\n","                        tokens,\n","                    )\n","                    logits = torch_outs.logits.swapaxes(1, 2)\n","                    lls = torch.zeros(tokens.shape[:2], device=self.device)\n","                    lls[:,1:] = -F.cross_entropy(logits[:,:,:-1], tokens[:,1:], reduction=\"none\")\n","\n","                clip_mask = torch.zeros_like(lls)\n","                for i in range(lls.shape[1]):\n","                    clip_mask[:,i] = ((i >= starts) & (i < ends))\n","\n","                seq_likelihoods = (lls * clip_mask).sum(1).numpy(force=True)\n","\n","            elif self.model_class == \"HyenaDNA\":\n","                encoded = self.tokenizer.batch_encode_plus(batch, return_tensors=\"pt\", padding=True)\n","                tokens = encoded[\"input_ids\"]\n","                attention_mask = encoded.get(\"attention_mask\")\n","                if self.start_token is not None:\n","                    starts = torch.where(tokens == self.start_token)[1] + 1\n","                else:\n","                    starts = 0\n","                if self.end_token is not None:\n","                    ends = torch.where(tokens == self.end_token)[1]\n","                else:\n","                    ends = attention_mask.sum(dim=1)\n","\n","                tokens = tokens.to(device=self.device)\n","                if attention_mask is not None:\n","                    attention_mask = attention_mask.to(device=self.device)\n","\n","                with torch.no_grad():\n","                    torch_outs = self.model(\n","                        tokens,\n","                    )\n","                    logits = torch_outs.logits.swapaxes(1, 2)\n","                    lls = torch.zeros(tokens.shape[:2], device=self.device)\n","                    lls[:,1:] = -F.cross_entropy(logits[:,:,:-1], tokens[:,1:], reduction=\"none\")\n","\n","                clip_mask = torch.zeros_like(lls)\n","                for i in range(lls.shape[1]):\n","                    clip_mask[:,i] = ((i >= starts) & (i < ends))\n","\n","                seq_likelihoods = (lls * clip_mask).sum(1).numpy(force=True)\n","\n","            elif self.model_class == \"DNABERT2\":\n","\n","                encoded = self.tokenizer.batch_encode_plus(batch, return_tensors=\"pt\", padding=True)\n","                tokens = encoded[\"input_ids\"]\n","                attention_mask = encoded.get(\"attention_mask\")\n","                if self.start_token is not None:\n","                    starts = torch.where(tokens == self.start_token)[1] + 1\n","                else:\n","                    starts = 0\n","                if self.end_token is not None:\n","                    ends = torch.where(tokens == self.end_token)[1]\n","                else:\n","                    ends = attention_mask.sum(dim=1)\n","\n","                tokens = tokens.to(device=self.device)\n","                if attention_mask is not None:\n","                    attention_mask = attention_mask.to(device=self.device)\n","                lls = torch.zeros(tokens.shape[:2], device=self.device)\n","                for i in range(tokens.shape[1]):\n","                    clip_mask = ((i >= starts) & (i < ends)).to(device=self.device)\n","                    masked_tokens = tokens.clone()\n","                    masked_tokens[:,i,...] = self.mask_token\n","                    with torch.no_grad():\n","                        torch_outs = self.model(\n","                            masked_tokens,\n","                            attention_mask=attention_mask,\n","                        )\n","                        logits = torch_outs.logits.swapaxes(1, 2)\n","                        tmp = -F.cross_entropy(logits, tokens, reduction=\"none\")\n","                        lls[:,i] = tmp[:,i] * clip_mask\n","\n","                seq_likelihoods = lls.sum(dim=1).numpy(force=True)\n","\n","            elif self.model_class == \"Caduceus\":\n","                encoded = self.tokenizer.batch_encode_plus(batch, return_tensors=\"pt\", padding=True)\n","                tokens = encoded[\"input_ids\"]\n","                attention_mask = encoded.get(\"attention_mask\")\n","                if self.start_token is not None:\n","                    starts = torch.where(tokens == self.start_token)[1] + 1\n","                else:\n","                    starts = 0\n","                if self.end_token is not None:\n","                    ends = torch.where(tokens == self.end_token)[1]\n","                else:\n","                    ends = attention_mask.sum(dim=1)\n","\n","                tokens = tokens.to(device=self.device)\n","                if attention_mask is not None:\n","                    attention_mask = attention_mask.to(device=self.device)\n","                lls = torch.zeros(tokens.shape[:2], device=self.device)\n","                for i in range(tokens.shape[1]):\n","                    clip_mask = ((i >= starts) & (i < ends)).to(device=self.device)\n","                    masked_tokens = tokens.clone()\n","                    masked_tokens[:,i,...] = self.mask_token\n","                    with torch.no_grad():\n","                        torch_outs = self.model(\n","                            masked_tokens,\n","                            attention_mask=attention_mask,\n","                        )\n","                        logits = torch_outs.logits.swapaxes(1, 2)\n","                        tmp = -F.cross_entropy(logits, tokens, reduction=\"none\")\n","                        lls[:,i] = tmp[:,i] * clip_mask\n","\n","                seq_likelihoods = lls.sum(dim=1).numpy(force=True)\n","            elif self.model_class == \"GENA-LM\":\n","                encoded = self.tokenizer.batch_encode_plus(batch, return_tensors=\"pt\", padding=True)\n","                tokens = encoded[\"input_ids\"]\n","                attention_mask = encoded.get(\"attention_mask\")\n","                if self.start_token is not None:\n","                    starts = torch.where(tokens == self.start_token)[1] + 1\n","                else:\n","                    starts = 0\n","                if self.end_token is not None:\n","                    ends = torch.where(tokens == self.end_token)[1]\n","                else:\n","                    ends = attention_mask.sum(dim=1)\n","\n","                tokens = tokens.to(device=self.device)\n","                if attention_mask is not None:\n","                    attention_mask = attention_mask.to(device=self.device)\n","                lls = torch.zeros(tokens.shape[:2], device=self.device)\n","                for i in range(tokens.shape[1]):\n","                    clip_mask = ((i >= starts) & (i < ends)).to(device=self.device)\n","                    masked_tokens = tokens.clone()\n","                    masked_tokens[:,i,...] = self.mask_token\n","                    with torch.no_grad():\n","                        torch_outs = self.model(\n","                            masked_tokens,\n","                            attention_mask=attention_mask,\n","                        )\n","                        logits = torch_outs.logits.swapaxes(1, 2)\n","                        tmp = -F.cross_entropy(logits, tokens, reduction=\"none\")\n","                        lls[:,i] = tmp[:,i] * clip_mask\n","\n","                seq_likelihoods = lls.sum(dim=1).numpy(force=True)\n","\n","            likelihoods.append(seq_likelihoods)\n","\n","        return np.concatenate(likelihoods)"],"metadata":{"id":"8gBjMWHX9zeP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# def MLP model\n","import torch\n","import torch.nn as nn\n","\n","# Define the MLP model for Binary Classification\n","class MLPBinary(nn.Module):\n","    def __init__(self, input_size):\n","        super(MLPBinary, self).__init__()\n","        self.fc1 = nn.Linear(input_size, 256)  # First hidden layer\n","        self.relu1 = nn.ReLU()\n","        self.fc2 = nn.Linear(256, 128) # Second hidden layer\n","        self.relu2 = nn.ReLU()\n","        self.fc3 = nn.Linear(128, 1)   # Output layer for binary classification\n","\n","    def forward(self, x):\n","        x = self.fc1(x)\n","        x = self.relu1(x)\n","        x = self.fc2(x)\n","        x = self.relu2(x)\n","        x = self.fc3(x) # No sigmoid here, as BCEWithLogitsLoss will be used\n","        return x"],"metadata":{"id":"X_qDNdG8IYmw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import (\n","    precision_recall_curve, average_precision_score,\n","    precision_score, recall_score, f1_score, accuracy_score\n",")\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import TensorDataset, DataLoader\n","\n","\n","def performance_cCREs(embeddings, label, batch_size=256, num_epochs=50):\n","    \"\"\"\n","    Train a small MLP classifier on embeddings with mini-batch SGD.\n","\n","    Parameters\n","    ----------\n","    embeddings : np.ndarray, shape (N, D)\n","    label      : array-like, shape (N,)\n","    batch_size : int\n","        Mini-batch size for training and evaluation.\n","    num_epochs : int\n","        Number of passes over the training set.\n","\n","    Returns\n","    -------\n","    prec_cls, rec_cls, f1_cls, acc_cls : float\n","        Precision, recall, F1 and accuracy on the held-out test set.\n","    \"\"\"\n","    X = embeddings\n","    y = np.asarray(label).astype(int)\n","\n","    # train/test split (stratified)\n","    X_train, X_test, y_train, y_test = train_test_split(\n","        X, y, test_size=0.3, random_state=40, stratify=y\n","    )\n","\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    # tensors\n","    X_train_t = torch.from_numpy(X_train).float()\n","    y_train_t = torch.from_numpy(y_train).float().view(-1, 1)\n","    X_test_t  = torch.from_numpy(X_test).float()\n","    y_test_t  = torch.from_numpy(y_test).float().view(-1, 1)\n","\n","    # datasets & loaders\n","    train_ds = TensorDataset(X_train_t, y_train_t)\n","    test_ds  = TensorDataset(X_test_t, y_test_t)\n","\n","    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,  drop_last=False)\n","    test_loader  = DataLoader(test_ds,  batch_size=batch_size, shuffle=False, drop_last=False)\n","\n","    # model\n","    input_size = X_train.shape[1]\n","    model = MLPBinary(input_size).to(device)  # your existing MLPBinary\n","\n","    # loss (no class weighting since data are balanced)\n","    criterion = nn.BCEWithLogitsLoss()\n","    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n","\n","    # --- training loop (batched) ---\n","    model.train()\n","    for epoch in range(num_epochs):\n","        for xb, yb in train_loader:\n","            xb = xb.to(device)\n","            yb = yb.to(device)\n","\n","            optimizer.zero_grad()\n","            logits = model(xb)\n","            loss = criterion(logits, yb)\n","            loss.backward()\n","            optimizer.step()\n","\n","    # --- evaluation (batched) ---\n","    model.eval()\n","    all_probs = []\n","    all_y_true = []\n","\n","    with torch.no_grad():\n","        for xb, yb in test_loader:\n","            xb = xb.to(device)\n","            logits = model(xb)\n","            probs = torch.sigmoid(logits).cpu().numpy().ravel()\n","\n","            all_probs.append(probs)\n","            all_y_true.append(yb.numpy().ravel())\n","\n","    probs  = np.concatenate(all_probs, axis=0)\n","    y_true = np.concatenate(all_y_true, axis=0).astype(int)\n","\n","    # metrics\n","    precision, recall, _ = precision_recall_curve(y_true, probs)\n","    ap = average_precision_score(y_true, probs)\n","\n","    y_pred = (probs >= 0.5).astype(int)\n","    prec_cls = precision_score(y_true, y_pred, zero_division=0)\n","    rec_cls  = recall_score(y_true, y_pred, zero_division=0)\n","    f1_cls   = f1_score(y_true, y_pred, zero_division=0)\n","    acc_cls  = accuracy_score(y_true, y_pred)\n","\n","    return prec_cls, rec_cls, f1_cls, acc_cls"],"metadata":{"id":"BQQwfFoIEOYN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","\n","# Path to your file\n","file_path = \"/content/drive/MyDrive/GitHub/Biological-Foundation-Model/Notebooks/EP Pair Evaluation/accessary_files/cCRE_pos_neg_dataset.csv\"\n","\n","# Read the compressed TSV file\n","df = pd.read_csv(file_path)"],"metadata":{"id":"cipCEbIC_KHS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Note:\n","## Mistral-DNA-v1-1.6B-hg38 not available on HuggingFace anymore\n","\n","embedding_extractor_hyenaDNA = dnalm_embedding_extraction(model_class=\"HyenaDNA\", model_name=\"hyenadna-large-1m-seqlen-hf\", device=torch.device(\"cuda\"))\n","embedding_extractor_mistral = dnalm_embedding_extraction(model_class=\"Mistral\", model_name=\"Mistral-DNA-v1-422M-hg38\", device=torch.device(\"cuda\"))\n","\n","embedding_extractor_dnabert2 = dnalm_embedding_extraction(model_class=\"DNABERT2\", model_name=\"DNABERT-2-117M\", device=torch.device(\"cuda\"))\n","embedding_extractor_nt = dnalm_embedding_extraction(model_class=\"Nucleotide Transformer\", model_name=\"nucleotide-transformer-v2-500m-multi-species\", device=torch.device(\"cuda\"))\n","# embedding_extractor_caduceus = dnalm_embedding_extraction(model_class=\"Caduceus\", model_name=\"caduceus-ps_seqlen-131k_d_model-256_n_layer-16\", device=torch.device(\"cuda\"))\n","embedding_extractor_genalm = dnalm_embedding_extraction(model_class=\"GENA-LM\", model_name=\"gena-lm-bert-large-t2t\", device=torch.device(\"cuda\"))"],"metadata":{"id":"fbgyy5yZ_MGK","executionInfo":{"status":"ok","timestamp":1763741244452,"user_tz":360,"elapsed":9860,"user":{"displayName":"Zhexuan Liu","userId":"18073944608501350772"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"96a956db-ff36-4878-d873-45c3220a4e2a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n","/root/.cache/huggingface/modules/transformers_modules/zhihan1996/DNABERT_hyphen_2_hyphen_117M/7bce263b15377fc15361f52cfab88f8b586abda0/bert_layers.py:126: UserWarning: Unable to import Triton; defaulting MosaicBERT attention implementation to pytorch (this will reduce throughput when using this model).\n","  warnings.warn(\n"]}]},{"cell_type":"markdown","source":["## Probed Absolute Accuracy"],"metadata":{"id":"vZrHWFPsmlxc"}},{"cell_type":"code","source":["df_sub = (\n","    df.groupby(\"label\", group_keys=False)\n","      .apply(lambda x: x.sample(n =5000, random_state=42))\n",")\n","# df_sub = df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cZWhPd2bAjzp","executionInfo":{"status":"ok","timestamp":1763741026146,"user_tz":360,"elapsed":426,"user":{"displayName":"Zhexuan Liu","userId":"18073944608501350772"}},"outputId":"62072fa8-7fa5-4872-d63e-fc6bede22ff2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-82786161.py:3: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n","  .apply(lambda x: x.sample(n =5000, random_state=42))\n"]}]},{"cell_type":"code","source":["for embedding_extractor in [embedding_extractor_mistral, embedding_extractor_genalm]: #, embedding_extractor_caduceus\n","    df_sub['embedding_mean'] = list(embedding_extractor.get_embedding(sequences=df_sub[\"sequence\"].tolist(), batch_size=100))\n","    prec_cls, rec_cls, f1_cls, acc_cls = performance_cCREs(np.vstack(df_sub['embedding_mean']).astype(np.float32), df_sub[\"label\"].astype(int).values, num_epochs = 100)\n","    print(embedding_extractor.model_class)\n","    print(f\"  Precision: {prec_cls:.3f}\")\n","    print(f\"  Recall   : {rec_cls:.3f}\")\n","    print(f\"  F1 score : {f1_cls:.3f}\")\n","    print(f\"  Accuracy : {acc_cls:.3f}\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ogrq2MWzGrxL","executionInfo":{"status":"ok","timestamp":1763741131644,"user_tz":360,"elapsed":99697,"user":{"displayName":"Zhexuan Liu","userId":"18073944608501350772"}},"outputId":"5c8b58cf-7a5e-4885-fb7e-72e5af9faba0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mistral\n","  Precision: 0.730\n","  Recall   : 0.806\n","  F1 score : 0.766\n","  Accuracy : 0.754\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py:1621: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["GENA-LM\n","  Precision: 0.871\n","  Recall   : 0.822\n","  F1 score : 0.846\n","  Accuracy : 0.850\n","\n"]}]},{"cell_type":"code","source":["for embedding_extractor in [embedding_extractor_hyenaDNA, embedding_extractor_dnabert2, embedding_extractor_nt]:\n","    df_sub['embedding_mean'] = list(embedding_extractor.get_embedding(sequences=df_sub[\"sequence\"].tolist(), batch_size=100))\n","    prec_cls, rec_cls, f1_cls, acc_cls = performance_cCREs(np.vstack(df_sub['embedding_mean']).astype(np.float32), df_sub[\"label\"].astype(int).values, num_epochs = 100)\n","    print(embedding_extractor.model_class)\n","    print(f\"  Precision: {prec_cls:.3f}\")\n","    print(f\"  Recall   : {rec_cls:.3f}\")\n","    print(f\"  F1 score : {f1_cls:.3f}\")\n","    print(f\"  Accuracy : {acc_cls:.3f}\\n\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fJ9Bq68jeik1","executionInfo":{"status":"ok","timestamp":1763698593522,"user_tz":360,"elapsed":162868,"user":{"displayName":"Zhexuan Liu","userId":"18073944608501350772"}},"outputId":"328c7817-6a57-4db7-8d15-66511e5e6fd9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["HyenaDNA\n","  Precision: 0.622\n","  Recall   : 0.789\n","  F1 score : 0.696\n","  Accuracy : 0.655\n","\n","DNABERT2\n","  Precision: 0.827\n","  Recall   : 0.753\n","  F1 score : 0.788\n","  Accuracy : 0.798\n","\n","Nucleotide Transformer\n","  Precision: 0.752\n","  Recall   : 0.763\n","  F1 score : 0.758\n","  Accuracy : 0.756\n","\n"]}]},{"cell_type":"markdown","source":["## Zero-shot accuracy"],"metadata":{"id":"3-JbuFofmr1L"}},{"cell_type":"code","source":["midpoint = df.shape[0] // 2\n","df_upper = df.iloc[:midpoint].copy().reset_index(drop=True)\n","df_lower = df.iloc[midpoint:].copy().reset_index(drop=True)\n","df_lower = df_lower.rename(columns={'sequence': 'sequence_shf', 'subtype': \"subtype_shf\", \"label\": \"label_shf\"})\n","df_combined = pd.concat([df_upper, df_lower], axis=1)\n","\n","df_sub = df_combined.sample(n=1000, random_state = 42)\n","# df_sub = df_combined"],"metadata":{"id":"uFIBnhgXmrV_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for embedding_extractor in [embedding_extractor_mistral, embedding_extractor_genalm]: #, embedding_extractor_caduceus\n","    df_sub['seq_pos_liklihood'] = list(embedding_extractor.get_likelihood(sequences=df_sub[\"sequence\"].tolist(), batch_size=50))\n","    df_sub['seq_neg_liklihood'] = list(embedding_extractor.get_likelihood(sequences=df_sub[\"sequence_shf\"].tolist(), batch_size=50))\n","    print(embedding_extractor.model_class)\n","    print((df_sub['seq_neg_liklihood']<=df_sub['seq_pos_liklihood']).sum() / df_sub.shape[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0x9JtJ2iGh5P","executionInfo":{"status":"ok","timestamp":1763742031513,"user_tz":360,"elapsed":787057,"user":{"displayName":"Zhexuan Liu","userId":"18073944608501350772"}},"outputId":"10c76819-7cf3-46ec-c31c-fe23e1b696c3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mistral\n","0.895\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py:1621: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["GENA-LM\n","0.941\n"]}]},{"cell_type":"code","source":["for embedding_extractor in [embedding_extractor_hyenaDNA, embedding_extractor_dnabert2, embedding_extractor_nt]:\n","    df_sub['seq_pos_liklihood'] = list(embedding_extractor.get_likelihood(sequences=df_sub[\"sequence\"].tolist(), batch_size=50))\n","    df_sub['seq_neg_liklihood'] = list(embedding_extractor.get_likelihood(sequences=df_sub[\"sequence_shf\"].tolist(), batch_size=50))\n","    print(embedding_extractor.model_class)\n","    print((df_sub['seq_neg_liklihood']<=df_sub['seq_pos_liklihood']).sum() / df_sub.shape[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KRcFxiOdmzgn","outputId":"10d108b0-8c4b-4242-fa5e-c6e7fa3c6b78","executionInfo":{"status":"ok","timestamp":1763706452494,"user_tz":360,"elapsed":1485531,"user":{"displayName":"Zhexuan Liu","userId":"18073944608501350772"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["HyenaDNA\n","0.857\n","DNABERT2\n","0.836\n","Nucleotide Transformer\n","0.709\n"]}]},{"cell_type":"code","source":["from google.colab import runtime\n","\n","runtime.unassign()"],"metadata":{"id":"yCoUOJYovCyr"},"execution_count":null,"outputs":[]}]}